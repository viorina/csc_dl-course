{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for first.W\n",
      "Gradient check passed!\n",
      "Checking gradient for first.B\n",
      "Gradient check passed!\n",
      "Checking gradient for second.W\n",
      "Gradient check passed!\n",
      "Checking gradient for second.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for first.W\n",
      "Gradient check passed!\n",
      "Checking gradient for first.B\n",
      "Gradient check passed!\n",
      "Checking gradient for second.W\n",
      "Gradient check passed!\n",
      "Checking gradient for second.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302662, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302842, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302493, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303531, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301383, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301490, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302752, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302385, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301478, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302296, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301885, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301752, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302206, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302096, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD())\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd23b015748>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAEXdJREFUeJzt3X/sXXV9x/Hny9ZuUxRaaRRbpOhwpmYI7K6oc+oClpbN1jkzS0SroMQ5kjlitiYsVotLFNQYF8LoNuaPOGCgzLrRlI6xOOPK+i0/CuVXa4PQirRaInNNYJX3/rifzsvX77ffy/fXbeH5SG6+55zP53PP+5zvuff1Pefc26aqkCTpeYMuQJJ0eDAQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpmTnoAp6JY489thYsWDDoMiTpiLJly5YfVdXcsfodUYGwYMEChoaGBl2GJB1Rkny/n35eMpIkAQaCJKkxECRJgIEgSWoMBEkS0GcgJFmS5P4kO5KsGqH9oiT3JNma5OYkJ/S0rUyyvT1W9iyflWRtkgeS3JfkDyZnkyRJ4zHmx06TzAAuB94G7AI2J1lXVff0dLsd6FTV/iR/BFwKvDvJHGA10AEK2NLGPgZcDOypqlcneR4wZ1K3TJL0jPTzPYRFwI6q2gmQ5BpgOfD/gVBVt/T03wSc26bPAjZW1b42diOwBLgaOA94TRv/FPCjCW3JoaxfBT+8a8qeXpKm1Mt+HZZ+espX088lo3nAwz3zu9qy0ZwPrD/U2CTHtPlLktyW5LokLx3pyZJckGQoydDevXv7KFeSNB6T+k3lJOfSvTz0lj7WOx/4blVdlOQi4LPAe4d3rKq1wFqATqdT4ypsGpJVko50/Zwh7AaO75mf35Y9TZIz6d4XWFZVT4wx9sfAfuAbbfl1wGnPqHJJ0qTqJxA2AyclOTHJLGAFsK63Q5JTgSvphsGenqYNwOIks5PMBhYDG6qqgG8Bb239zqDnnoQkafqNecmoqg4kuZDum/sM4Kqq2pZkDTBUVeuAy4CjgOuSADxUVcuqal+SS+iGCsCagzeYgT8HvprkC8Be4AOTumWSpGck3T/WjwydTqf8104l6ZlJsqWqOmP185vKkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJKDPQEiyJMn9SXYkWTVC+0VJ7kmyNcnNSU7oaVuZZHt7rBxh7Lokd09sMyRJEzVmICSZAVwOLAUWAuckWTis2+1Ap6pOBq4HLm1j5wCrgdOBRcDqJLN7nvudwE8nYTskSRPUzxnCImBHVe2sqieBa4DlvR2q6paq2t9mNwHz2/RZwMaq2ldVjwEbgSUASY4CLgI+NfHNkCRNVD+BMA94uGd+V1s2mvOB9X2MvQT4HLAfSdLATepN5STnAh3gsjH6nQK8qqpu6OM5L0gylGRo7969k1SpJGm4fgJhN3B8z/z8tuxpkpwJXAwsq6onxhj7BqCT5EHgO8Crk/z7SCuvqrVV1amqzty5c/soV5I0Hv0EwmbgpCQnJpkFrADW9XZIcipwJd0w2NPTtAFYnGR2u5m8GNhQVVdU1curagHwJuCBqnrrxDdHkjReM8fqUFUHklxI9819BnBVVW1LsgYYqqp1dC8RHQVclwTgoapaVlX7klxCN1QA1lTVvinZEknShKSqBl1D3zqdTg0NDQ26DEk6oiTZUlWdsfr5TWVJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkS0GcgJFmS5P4kO5KsGqH9oiT3JNma5OYkJ/S0rUyyvT1WtmUvSPIvSe5Lsi3JpydvkyRJ4zFmICSZAVwOLAUWAuckWTis2+1Ap6pOBq4HLm1j5wCrgdOBRcDqJLPbmM9W1WuAU4HfSrJ0ErZHkjRO/ZwhLAJ2VNXOqnoSuAZY3tuhqm6pqv1tdhMwv02fBWysqn1V9RiwEVhSVfur6pY29kngtp4xkqQB6CcQ5gEP98zvastGcz6wvt+xSY4B3g7c3EctkqQpMnMynyzJuUAHeEuf/WcCVwNfrKqdo/S5ALgA4BWveMUkVSpJGq6fM4TdwPE98/PbsqdJciZwMbCsqp7oc+xaYHtVfWG0lVfV2qrqVFVn7ty5fZQrSRqPfgJhM3BSkhOTzAJWAOt6OyQ5FbiSbhjs6WnaACxOMrvdTF7clpHkU8DRwEcnvhmSpIkaMxCq6gBwId038nuBf6yqbUnWJFnWul0GHAVcl+SOJOva2H3AJXRDZTOwpqr2JZlP92xiIXBbG/PByd44SVL/UlWDrqFvnU6nhoaGBl2GJB1Rkmypqs5Y/fymsiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJTV+BkGRJkvuT7EiyaoT2i5Lck2RrkpuTnNDTtjLJ9vZY2bP8N5Lc1Z7zi0kyOZskSRqPMQMhyQzgcmApsBA4J8nCYd1uBzpVdTJwPXBpGzsHWA2cDiwCVieZ3cZcAXwIOKk9lkx4ayRJ49bPGcIiYEdV7ayqJ4FrgOW9Harqlqra32Y3AfPb9FnAxqraV1WPARuBJUmOA15cVZuqqoCvAO+YhO2RJI1TP4EwD3i4Z35XWzaa84H1Y4yd16bHfM4kFyQZSjK0d+/ePsqVJI3HpN5UTnIu0AEum6znrKq1VdWpqs7cuXMn62klScP0Ewi7geN75ue3ZU+T5EzgYmBZVT0xxtjd/Pyy0qjPKUmaPv0EwmbgpCQnJpkFrADW9XZIcipwJd0w2NPTtAFYnGR2u5m8GNhQVY8Ajyd5fft00fuAb07C9kiSxmnmWB2q6kCSC+m+uc8ArqqqbUnWAENVtY7uJaKjgOvap0cfqqplVbUvySV0QwVgTVXta9MfAb4E/Ardew7rkSQNTLof8jkydDqdGhoaGnQZknRESbKlqjpj9fObypIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiSgz0BIsiTJ/Ul2JFk1Qvubk9yW5ECSdw1r+0ySu9vj3T3Lz2hj7kjynSS/OvHNkSSN15iBkGQGcDmwFFgInJNk4bBuDwHvB/5h2NjfBU4DTgFOBz6W5MWt+QrgPVV1Shv3F+PfDEnSRPVzhrAI2FFVO6vqSeAaYHlvh6p6sKq2Ak8NG7sQ+HZVHaiq/wG2AksODgMOhsPRwA/GuQ2SpEnQTyDMAx7umd/VlvXjTmBJkhckORb4HeD41vZB4MYku4D3Ap/u8zklSVNgSm8qV9VNwI3Ad4Grgf8Eftaa/xQ4u6rmA38PfH6k50hyQZKhJEN79+6dynIl6Tmtn0DYzc//qgeY35b1par+sqpOqaq3AQEeSDIXeF1V3dq6XQu8cZTxa6uqU1WduXPn9rtaSdIz1E8gbAZOSnJiklnACmBdP0+eZEaSl7Tpk4GTgZuAx4Cjk7y6dX0bcO8zLV6SNHlmjtWhqg4kuRDYAMwArqqqbUnWAENVtS7JbwI3ALOBtyf5ZFW9Fng+8B9JAB4Hzq2qAwBJPgR8PclTdAPivCnYPklSn1JVg66hb51Op4aGhgZdhiQdUZJsqarOWP38prIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUzBx0AdPhk9/axj0/eHzQZUjSuCx8+YtZ/fbXTvl6PEOQJAHPkTOE6UhWSTrSeYYgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJElNqmrQNfQtyV7g++Mcfizwo0ksZ7JZ38RY38RY38Qc7vWdUFVzx+p0RAXCRCQZqqrOoOsYjfVNjPVNjPVNzOFeX7+8ZCRJAgwESVLzXAqEtYMuYAzWNzHWNzHWNzGHe319ec7cQ5AkHdpz6QxBknQIz7pASLIkyf1JdiRZNUL7LyW5trXfmmTBNNZ2fJJbktyTZFuSPxmhz1uT/CTJHe3x8emqr63/wSR3tXUPjdCeJF9s+29rktOmsbZf69kvdyR5PMlHh/WZ1v2X5Koke5Lc3bNsTpKNSba3n7NHGbuy9dmeZOU01ndZkvva7++GJMeMMvaQx8IU1veJJLt7fodnjzL2kK/1Kazv2p7aHkxyxyhjp3z/TbqqetY8gBnA94BXArOAO4GFw/p8BPjrNr0CuHYa6zsOOK1Nvwh4YIT63gr88wD34YPAsYdoPxtYDwR4PXDrAH/XP6T7+eqB7T/gzcBpwN09yy4FVrXpVcBnRhg3B9jZfs5u07Onqb7FwMw2/ZmR6uvnWJjC+j4BfKyP3/8hX+tTVd+w9s8BHx/U/pvsx7PtDGERsKOqdlbVk8A1wPJhfZYDX27T1wNnJMl0FFdVj1TVbW36v4F7gXnTse5JtBz4SnVtAo5JctwA6jgD+F5VjfeLipOiqr4N7Bu2uPcY+zLwjhGGngVsrKp9VfUYsBFYMh31VdVNVXWgzW4C5k/2evs1yv7rRz+v9Qk7VH3tfeMPgasne72D8mwLhHnAwz3zu/jFN9z/79NeFD8BXjIt1fVol6pOBW4dofkNSe5Msj7JdP//nwXclGRLkgtGaO9nH0+HFYz+Qhzk/gN4aVU90qZ/CLx0hD6Hy348j+4Z30jGOham0oXtktZVo1xyOxz2328Dj1bV9lHaB7n/xuXZFghHhCRHAV8HPlpVjw9rvo3uZZDXAX8F/NM0l/emqjoNWAr8cZI3T/P6x5RkFrAMuG6E5kHvv6ep7rWDw/KjfEkuBg4AXxuly6COhSuAVwGnAI/QvSxzODqHQ58dHPavpeGebYGwGzi+Z35+WzZinyQzgaOBH09Ldd11Pp9uGHytqr4xvL2qHq+qn7bpG4HnJzl2uuqrqt3t5x7gBrqn5r362cdTbSlwW1U9Orxh0PuvefTgZbT2c88IfQa6H5O8H/g94D0ttH5BH8fClKiqR6vqZ1X1FPA3o6x30PtvJvBO4NrR+gxq/03Esy0QNgMnJTmx/RW5Alg3rM864OAnOt4F/NtoL4jJ1q45/h1wb1V9fpQ+Lzt4TyPJIrq/o2kJrCQvTPKig9N0bz7ePazbOuB97dNGrwd+0nN5ZLqM+pfZIPdfj95jbCXwzRH6bAAWJ5ndLoksbsumXJIlwJ8By6pq/yh9+jkWpqq+3ntSvz/Kevt5rU+lM4H7qmrXSI2D3H8TMui72pP9oPspmAfofgLh4rZsDd2DH+CX6V5q2AH8F/DKaaztTXQvH2wF7miPs4EPAx9ufS4EttH91MQm4I3TWN8r23rvbDUc3H+99QW4vO3fu4DONP9+X0j3Df7onmUD2390g+kR4H/pXsc+n+49qZuB7cC/AnNa3w7wtz1jz2vH4Q7gA9NY3w66198PHoMHP3X3cuDGQx0L01TfV9uxtZXum/xxw+tr87/wWp+O+tryLx085nr6Tvv+m+yH31SWJAHPvktGkqRxMhAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAfB/wZAUEaaenHkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.313882, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295358, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.307993, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289319, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271916, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.315795, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292668, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269878, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.330227, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264027, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282077, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271336, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273658, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273410, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303250, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.199527, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.320560, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.320490, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285667, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281102, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305996, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296499, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262741, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.319219, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271805, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239979, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302256, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.284094, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257180, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275231, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260643, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291107, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287711, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304130, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.231559, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.351386, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319573, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.320329, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.281196, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.331344, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.253567, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.277398, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.208324, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.227107, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.030399, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.122037, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.873827, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.921337, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.775937, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.085005, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.817975, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.106093, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.368405, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.121562, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.584942, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.947412, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.726571, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.755935, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.838188, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.228244, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.111308, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.721648, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.141381, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.780013, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.931743, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.818621, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.506972, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.878513, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.240420, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.671532, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.845841, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.834279, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.885771, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.074952, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.370104, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.781072, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.625678, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.660183, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.643286, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.394114, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.363041, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.179197, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.358390, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.700854, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 0.820708, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.347979, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.072846, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.454723, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.457327, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.650330, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.250412, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.266090, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.754367, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.264470, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.286876, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.983230, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.625142, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.743834, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.798522, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.195205, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.370609, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.355454, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.339356, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.902429, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.007827, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.248346, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.020295, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.227555, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.583440, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.418328, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.976943, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.345475, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.594437, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.137630, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.308410, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.031786, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.976830, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.390303, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.050395, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.310691, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.291077, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.373447, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.941530, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.323198, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.665275, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.142289, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.566841, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.574824, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.260416, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.204598, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.411905, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.288506, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.310923, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.263796, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.295994, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.217932, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.566314, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.964486, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.273538, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.903287, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.255306, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.537363, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.532080, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.179853, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.317676, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.240337, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.404147, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.611186, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.149537, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.274862, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.099579, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.087049, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.599867, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.228474, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.152394, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.456976, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.411732, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.691240, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.442164, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.419263, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.398199, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.127397, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.402695, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.368445, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.415614, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.284091, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.669777, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.128049, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.142164, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.216886, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.333346, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.449104, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.430849, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.343800, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.463846, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.292512, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.389042, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.582065, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.435610, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.253898, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.222749, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.362537, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.275578, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.130505, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.434080, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.317628, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.213827, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.179297, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.159438, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.956140, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.875569, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.391688, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.876594, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.360227, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.715871, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.496604, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.058878, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.287590, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.225374, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.057979, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.085802, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.073897, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.068703, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.070204, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.071379, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 200, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate=0.1, reg_strength=0.001, n_layers=100\n",
      "Loss: 1.865112, Train accuracy: 0.373556, val accuracy: 0.392000\n",
      "Loss: 1.530418, Train accuracy: 0.529556, val accuracy: 0.509000\n",
      "Loss: 1.502702, Train accuracy: 0.561111, val accuracy: 0.560000\n",
      "Loss: 1.737578, Train accuracy: 0.591222, val accuracy: 0.579000\n",
      "Loss: 1.736214, Train accuracy: 0.620444, val accuracy: 0.598000\n",
      "Loss: 1.693982, Train accuracy: 0.599444, val accuracy: 0.591000\n",
      "Loss: 1.416750, Train accuracy: 0.674000, val accuracy: 0.636000\n",
      "Loss: 1.853064, Train accuracy: 0.533444, val accuracy: 0.498000\n",
      "Loss: 2.042062, Train accuracy: 0.601667, val accuracy: 0.578000\n",
      "Loss: 1.504536, Train accuracy: 0.620778, val accuracy: 0.572000\n",
      "Loss: 2.199856, Train accuracy: 0.627444, val accuracy: 0.590000\n",
      "Loss: 1.428801, Train accuracy: 0.605778, val accuracy: 0.572000\n",
      "Loss: 2.347654, Train accuracy: 0.605000, val accuracy: 0.574000\n",
      "Loss: 1.827153, Train accuracy: 0.631556, val accuracy: 0.608000\n",
      "Loss: 1.425484, Train accuracy: 0.659333, val accuracy: 0.621000\n",
      "Loss: 1.697934, Train accuracy: 0.663667, val accuracy: 0.633000\n",
      "Loss: 1.597261, Train accuracy: 0.637333, val accuracy: 0.618000\n",
      "Loss: 2.134745, Train accuracy: 0.665778, val accuracy: 0.636000\n",
      "Loss: 1.650373, Train accuracy: 0.640889, val accuracy: 0.597000\n",
      "Loss: 1.885242, Train accuracy: 0.626000, val accuracy: 0.604000\n",
      "Loss: 1.650862, Train accuracy: 0.644111, val accuracy: 0.606000\n",
      "Loss: 1.187127, Train accuracy: 0.660778, val accuracy: 0.630000\n",
      "Loss: 1.801534, Train accuracy: 0.653444, val accuracy: 0.608000\n",
      "Loss: 1.613028, Train accuracy: 0.639444, val accuracy: 0.607000\n",
      "Loss: 1.333387, Train accuracy: 0.656000, val accuracy: 0.622000\n",
      "learning_rate=0.1, reg_strength=0.001, n_layers=200\n",
      "Loss: 1.662597, Train accuracy: 0.389333, val accuracy: 0.390000\n",
      "Loss: 1.428037, Train accuracy: 0.553222, val accuracy: 0.526000\n",
      "Loss: 1.679528, Train accuracy: 0.543778, val accuracy: 0.554000\n",
      "Loss: 1.836146, Train accuracy: 0.593889, val accuracy: 0.563000\n",
      "Loss: 1.656872, Train accuracy: 0.630111, val accuracy: 0.607000\n",
      "Loss: 1.364273, Train accuracy: 0.643222, val accuracy: 0.625000\n",
      "Loss: 1.641696, Train accuracy: 0.628111, val accuracy: 0.613000\n",
      "Loss: 2.214288, Train accuracy: 0.609111, val accuracy: 0.576000\n",
      "Loss: 1.877968, Train accuracy: 0.654444, val accuracy: 0.630000\n",
      "Loss: 1.795045, Train accuracy: 0.626222, val accuracy: 0.602000\n",
      "Loss: 2.223265, Train accuracy: 0.572111, val accuracy: 0.540000\n",
      "Loss: 1.601752, Train accuracy: 0.581222, val accuracy: 0.572000\n",
      "Loss: 1.773972, Train accuracy: 0.624222, val accuracy: 0.585000\n",
      "Loss: 1.342606, Train accuracy: 0.654222, val accuracy: 0.616000\n",
      "Loss: 1.640864, Train accuracy: 0.656000, val accuracy: 0.616000\n",
      "Loss: 1.137616, Train accuracy: 0.669444, val accuracy: 0.632000\n",
      "Loss: 1.196289, Train accuracy: 0.631556, val accuracy: 0.597000\n",
      "Loss: 1.483734, Train accuracy: 0.640556, val accuracy: 0.586000\n",
      "Loss: 1.919179, Train accuracy: 0.627111, val accuracy: 0.609000\n",
      "Loss: 1.806984, Train accuracy: 0.616667, val accuracy: 0.579000\n",
      "Loss: 1.659223, Train accuracy: 0.631889, val accuracy: 0.627000\n",
      "Loss: 1.936529, Train accuracy: 0.597333, val accuracy: 0.564000\n",
      "Loss: 2.309754, Train accuracy: 0.641556, val accuracy: 0.608000\n",
      "Loss: 1.730200, Train accuracy: 0.675778, val accuracy: 0.643000\n",
      "Loss: 2.061958, Train accuracy: 0.676556, val accuracy: 0.641000\n",
      "learning_rate=0.1, reg_strength=0.001, n_layers=500\n",
      "Loss: 1.412631, Train accuracy: 0.381889, val accuracy: 0.398000\n",
      "Loss: 1.193209, Train accuracy: 0.560222, val accuracy: 0.546000\n",
      "Loss: 1.723779, Train accuracy: 0.572222, val accuracy: 0.562000\n",
      "Loss: 1.728077, Train accuracy: 0.566778, val accuracy: 0.555000\n",
      "Loss: 1.830989, Train accuracy: 0.599556, val accuracy: 0.584000\n",
      "Loss: 1.715696, Train accuracy: 0.606444, val accuracy: 0.577000\n",
      "Loss: 2.755153, Train accuracy: 0.624889, val accuracy: 0.566000\n",
      "Loss: 1.570587, Train accuracy: 0.601000, val accuracy: 0.589000\n",
      "Loss: 2.134758, Train accuracy: 0.647222, val accuracy: 0.634000\n",
      "Loss: 1.639343, Train accuracy: 0.648000, val accuracy: 0.612000\n",
      "Loss: 1.745556, Train accuracy: 0.641889, val accuracy: 0.616000\n",
      "Loss: 2.496345, Train accuracy: 0.580000, val accuracy: 0.568000\n",
      "Loss: 1.933481, Train accuracy: 0.649889, val accuracy: 0.621000\n",
      "Loss: 2.104654, Train accuracy: 0.613444, val accuracy: 0.590000\n",
      "Loss: 2.730230, Train accuracy: 0.608556, val accuracy: 0.594000\n",
      "Loss: 2.148426, Train accuracy: 0.676111, val accuracy: 0.649000\n",
      "Loss: 2.003007, Train accuracy: 0.665444, val accuracy: 0.621000\n",
      "Loss: 1.610008, Train accuracy: 0.684667, val accuracy: 0.661000\n",
      "Loss: 1.572642, Train accuracy: 0.621444, val accuracy: 0.602000\n",
      "Loss: 1.907369, Train accuracy: 0.660889, val accuracy: 0.647000\n",
      "Loss: 2.137621, Train accuracy: 0.647889, val accuracy: 0.591000\n",
      "Loss: 1.808225, Train accuracy: 0.596778, val accuracy: 0.557000\n",
      "Loss: 1.669392, Train accuracy: 0.648444, val accuracy: 0.605000\n",
      "Loss: 1.827109, Train accuracy: 0.648778, val accuracy: 0.613000\n",
      "Loss: 1.223025, Train accuracy: 0.636111, val accuracy: 0.596000\n",
      "learning_rate=0.1, reg_strength=0.0001, n_layers=100\n",
      "Loss: 1.905333, Train accuracy: 0.375000, val accuracy: 0.408000\n",
      "Loss: 1.303189, Train accuracy: 0.518667, val accuracy: 0.499000\n",
      "Loss: 1.288509, Train accuracy: 0.592222, val accuracy: 0.571000\n",
      "Loss: 1.776878, Train accuracy: 0.656778, val accuracy: 0.625000\n",
      "Loss: 1.302727, Train accuracy: 0.617778, val accuracy: 0.583000\n",
      "Loss: 1.527679, Train accuracy: 0.648333, val accuracy: 0.616000\n",
      "Loss: 1.941280, Train accuracy: 0.662667, val accuracy: 0.626000\n",
      "Loss: 0.542475, Train accuracy: 0.683444, val accuracy: 0.656000\n",
      "Loss: 0.976204, Train accuracy: 0.705222, val accuracy: 0.648000\n",
      "Loss: 1.165454, Train accuracy: 0.693111, val accuracy: 0.629000\n",
      "Loss: 1.062772, Train accuracy: 0.706000, val accuracy: 0.637000\n",
      "Loss: 0.962283, Train accuracy: 0.702333, val accuracy: 0.634000\n",
      "Loss: 1.196774, Train accuracy: 0.742556, val accuracy: 0.665000\n",
      "Loss: 1.137729, Train accuracy: 0.777000, val accuracy: 0.700000\n",
      "Loss: 1.661547, Train accuracy: 0.728556, val accuracy: 0.653000\n",
      "Loss: 1.634026, Train accuracy: 0.768444, val accuracy: 0.698000\n",
      "Loss: 1.214919, Train accuracy: 0.722333, val accuracy: 0.649000\n",
      "Loss: 0.651919, Train accuracy: 0.736778, val accuracy: 0.675000\n",
      "Loss: 0.939423, Train accuracy: 0.753111, val accuracy: 0.658000\n",
      "Loss: 0.878515, Train accuracy: 0.762889, val accuracy: 0.680000\n",
      "Loss: 0.724023, Train accuracy: 0.766333, val accuracy: 0.658000\n",
      "Loss: 0.971825, Train accuracy: 0.789222, val accuracy: 0.670000\n",
      "Loss: 1.405615, Train accuracy: 0.762333, val accuracy: 0.679000\n",
      "Loss: 1.240715, Train accuracy: 0.771222, val accuracy: 0.687000\n",
      "Loss: 1.031421, Train accuracy: 0.768111, val accuracy: 0.679000\n",
      "learning_rate=0.1, reg_strength=0.0001, n_layers=200\n",
      "Loss: 2.313682, Train accuracy: 0.333889, val accuracy: 0.346000\n",
      "Loss: 1.723060, Train accuracy: 0.491111, val accuracy: 0.476000\n",
      "Loss: 2.664013, Train accuracy: 0.579222, val accuracy: 0.579000\n",
      "Loss: 1.360100, Train accuracy: 0.568556, val accuracy: 0.554000\n",
      "Loss: 1.617466, Train accuracy: 0.687444, val accuracy: 0.632000\n",
      "Loss: 1.675777, Train accuracy: 0.672778, val accuracy: 0.619000\n",
      "Loss: 1.971487, Train accuracy: 0.690667, val accuracy: 0.639000\n",
      "Loss: 0.905001, Train accuracy: 0.723667, val accuracy: 0.678000\n",
      "Loss: 1.598762, Train accuracy: 0.659667, val accuracy: 0.619000\n",
      "Loss: 1.107117, Train accuracy: 0.741444, val accuracy: 0.705000\n",
      "Loss: 1.320726, Train accuracy: 0.706222, val accuracy: 0.635000\n",
      "Loss: 1.361013, Train accuracy: 0.734556, val accuracy: 0.673000\n",
      "Loss: 1.524576, Train accuracy: 0.725556, val accuracy: 0.666000\n",
      "Loss: 1.826000, Train accuracy: 0.751000, val accuracy: 0.671000\n",
      "Loss: 1.810411, Train accuracy: 0.746889, val accuracy: 0.670000\n",
      "Loss: 1.535062, Train accuracy: 0.769333, val accuracy: 0.673000\n",
      "Loss: 1.470281, Train accuracy: 0.752111, val accuracy: 0.672000\n",
      "Loss: 0.857454, Train accuracy: 0.775667, val accuracy: 0.676000\n",
      "Loss: 1.194779, Train accuracy: 0.733889, val accuracy: 0.658000\n",
      "Loss: 1.545299, Train accuracy: 0.774778, val accuracy: 0.692000\n",
      "Loss: 1.376128, Train accuracy: 0.752667, val accuracy: 0.671000\n",
      "Loss: 1.155996, Train accuracy: 0.754333, val accuracy: 0.673000\n",
      "Loss: 0.766541, Train accuracy: 0.778778, val accuracy: 0.669000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.708163, Train accuracy: 0.777778, val accuracy: 0.691000\n",
      "Loss: 1.970491, Train accuracy: 0.787111, val accuracy: 0.695000\n",
      "learning_rate=0.1, reg_strength=0.0001, n_layers=500\n",
      "Loss: 1.757507, Train accuracy: 0.389889, val accuracy: 0.389000\n",
      "Loss: 1.424207, Train accuracy: 0.524556, val accuracy: 0.503000\n",
      "Loss: 1.196019, Train accuracy: 0.585444, val accuracy: 0.554000\n",
      "Loss: 1.972832, Train accuracy: 0.613333, val accuracy: 0.564000\n",
      "Loss: 1.987790, Train accuracy: 0.629667, val accuracy: 0.570000\n",
      "Loss: 1.337781, Train accuracy: 0.687889, val accuracy: 0.636000\n",
      "Loss: 1.623218, Train accuracy: 0.692222, val accuracy: 0.644000\n",
      "Loss: 1.526388, Train accuracy: 0.676556, val accuracy: 0.625000\n",
      "Loss: 1.982214, Train accuracy: 0.705889, val accuracy: 0.650000\n",
      "Loss: 1.076568, Train accuracy: 0.693000, val accuracy: 0.652000\n",
      "Loss: 1.384849, Train accuracy: 0.703444, val accuracy: 0.653000\n",
      "Loss: 1.861816, Train accuracy: 0.649778, val accuracy: 0.597000\n",
      "Loss: 0.622835, Train accuracy: 0.728333, val accuracy: 0.660000\n",
      "Loss: 0.728030, Train accuracy: 0.749667, val accuracy: 0.659000\n",
      "Loss: 1.299435, Train accuracy: 0.750556, val accuracy: 0.667000\n",
      "Loss: 1.094578, Train accuracy: 0.757778, val accuracy: 0.671000\n",
      "Loss: 2.197561, Train accuracy: 0.735333, val accuracy: 0.648000\n",
      "Loss: 2.042226, Train accuracy: 0.753000, val accuracy: 0.654000\n",
      "Loss: 1.926453, Train accuracy: 0.759333, val accuracy: 0.677000\n",
      "Loss: 1.531211, Train accuracy: 0.725111, val accuracy: 0.640000\n",
      "Loss: 1.399525, Train accuracy: 0.748778, val accuracy: 0.678000\n",
      "Loss: 1.528854, Train accuracy: 0.730444, val accuracy: 0.627000\n",
      "Loss: 1.401476, Train accuracy: 0.775778, val accuracy: 0.668000\n",
      "Loss: 1.413944, Train accuracy: 0.785667, val accuracy: 0.679000\n",
      "Loss: 1.227754, Train accuracy: 0.748000, val accuracy: 0.653000\n",
      "learning_rate=0.1, reg_strength=1e-05, n_layers=100\n",
      "Loss: 1.807364, Train accuracy: 0.342778, val accuracy: 0.335000\n",
      "Loss: 1.659075, Train accuracy: 0.550556, val accuracy: 0.550000\n",
      "Loss: 1.839039, Train accuracy: 0.595444, val accuracy: 0.583000\n",
      "Loss: 0.996534, Train accuracy: 0.617444, val accuracy: 0.595000\n",
      "Loss: 1.555667, Train accuracy: 0.651444, val accuracy: 0.637000\n",
      "Loss: 1.368017, Train accuracy: 0.675000, val accuracy: 0.645000\n",
      "Loss: 0.976956, Train accuracy: 0.679222, val accuracy: 0.630000\n",
      "Loss: 0.765322, Train accuracy: 0.705222, val accuracy: 0.640000\n",
      "Loss: 1.528841, Train accuracy: 0.683333, val accuracy: 0.614000\n",
      "Loss: 1.791363, Train accuracy: 0.710111, val accuracy: 0.632000\n",
      "Loss: 0.942120, Train accuracy: 0.733889, val accuracy: 0.671000\n",
      "Loss: 0.862907, Train accuracy: 0.732000, val accuracy: 0.660000\n",
      "Loss: 0.987686, Train accuracy: 0.722111, val accuracy: 0.646000\n",
      "Loss: 1.166517, Train accuracy: 0.740778, val accuracy: 0.668000\n",
      "Loss: 0.887471, Train accuracy: 0.773111, val accuracy: 0.688000\n",
      "Loss: 1.025742, Train accuracy: 0.767111, val accuracy: 0.692000\n",
      "Loss: 1.052298, Train accuracy: 0.762778, val accuracy: 0.671000\n",
      "Loss: 0.795396, Train accuracy: 0.753556, val accuracy: 0.682000\n",
      "Loss: 0.858407, Train accuracy: 0.798444, val accuracy: 0.697000\n",
      "Loss: 0.615817, Train accuracy: 0.781889, val accuracy: 0.697000\n",
      "Loss: 1.203538, Train accuracy: 0.786333, val accuracy: 0.692000\n",
      "Loss: 0.823073, Train accuracy: 0.802444, val accuracy: 0.716000\n",
      "Loss: 0.650873, Train accuracy: 0.810333, val accuracy: 0.690000\n",
      "Loss: 0.624259, Train accuracy: 0.803222, val accuracy: 0.689000\n",
      "Loss: 1.156019, Train accuracy: 0.820889, val accuracy: 0.711000\n",
      "learning_rate=0.1, reg_strength=1e-05, n_layers=200\n",
      "Loss: 1.762538, Train accuracy: 0.398111, val accuracy: 0.414000\n",
      "Loss: 1.790556, Train accuracy: 0.504889, val accuracy: 0.510000\n",
      "Loss: 1.729546, Train accuracy: 0.590667, val accuracy: 0.590000\n",
      "Loss: 0.782571, Train accuracy: 0.671444, val accuracy: 0.633000\n",
      "Loss: 1.145090, Train accuracy: 0.662222, val accuracy: 0.610000\n",
      "Loss: 0.675479, Train accuracy: 0.660889, val accuracy: 0.619000\n",
      "Loss: 0.849616, Train accuracy: 0.685444, val accuracy: 0.648000\n",
      "Loss: 1.035935, Train accuracy: 0.708111, val accuracy: 0.630000\n",
      "Loss: 1.261867, Train accuracy: 0.715556, val accuracy: 0.659000\n",
      "Loss: 0.785068, Train accuracy: 0.704778, val accuracy: 0.628000\n",
      "Loss: 1.859291, Train accuracy: 0.733111, val accuracy: 0.661000\n",
      "Loss: 0.726997, Train accuracy: 0.777667, val accuracy: 0.685000\n",
      "Loss: 0.464704, Train accuracy: 0.773889, val accuracy: 0.681000\n",
      "Loss: 1.571722, Train accuracy: 0.763778, val accuracy: 0.668000\n",
      "Loss: 0.758625, Train accuracy: 0.774222, val accuracy: 0.681000\n",
      "Loss: 1.132315, Train accuracy: 0.749778, val accuracy: 0.662000\n",
      "Loss: 0.724381, Train accuracy: 0.793889, val accuracy: 0.675000\n",
      "Loss: 1.124847, Train accuracy: 0.763111, val accuracy: 0.662000\n",
      "Loss: 1.020548, Train accuracy: 0.764444, val accuracy: 0.659000\n",
      "Loss: 0.956415, Train accuracy: 0.780778, val accuracy: 0.678000\n",
      "Loss: 0.923705, Train accuracy: 0.823889, val accuracy: 0.679000\n",
      "Loss: 0.724283, Train accuracy: 0.801778, val accuracy: 0.667000\n",
      "Loss: 1.299874, Train accuracy: 0.794333, val accuracy: 0.679000\n",
      "Loss: 0.848659, Train accuracy: 0.814444, val accuracy: 0.678000\n",
      "Loss: 0.926165, Train accuracy: 0.803111, val accuracy: 0.699000\n",
      "learning_rate=0.1, reg_strength=1e-05, n_layers=500\n",
      "Loss: 1.796453, Train accuracy: 0.392889, val accuracy: 0.409000\n",
      "Loss: 1.599533, Train accuracy: 0.518889, val accuracy: 0.501000\n",
      "Loss: 1.247149, Train accuracy: 0.612444, val accuracy: 0.587000\n",
      "Loss: 1.186598, Train accuracy: 0.623000, val accuracy: 0.589000\n",
      "Loss: 1.776903, Train accuracy: 0.650222, val accuracy: 0.590000\n",
      "Loss: 1.139748, Train accuracy: 0.681222, val accuracy: 0.622000\n",
      "Loss: 1.347452, Train accuracy: 0.692889, val accuracy: 0.622000\n",
      "Loss: 1.687723, Train accuracy: 0.705556, val accuracy: 0.650000\n",
      "Loss: 1.625403, Train accuracy: 0.712000, val accuracy: 0.638000\n",
      "Loss: 1.419662, Train accuracy: 0.716444, val accuracy: 0.650000\n",
      "Loss: 1.224617, Train accuracy: 0.688444, val accuracy: 0.622000\n",
      "Loss: 1.443336, Train accuracy: 0.749333, val accuracy: 0.652000\n",
      "Loss: 1.306720, Train accuracy: 0.769111, val accuracy: 0.670000\n",
      "Loss: 0.894906, Train accuracy: 0.728333, val accuracy: 0.646000\n",
      "Loss: 1.255632, Train accuracy: 0.731889, val accuracy: 0.648000\n",
      "Loss: 1.759763, Train accuracy: 0.751889, val accuracy: 0.658000\n",
      "Loss: 1.292222, Train accuracy: 0.769333, val accuracy: 0.667000\n",
      "Loss: 1.089155, Train accuracy: 0.734000, val accuracy: 0.660000\n",
      "Loss: 3.031264, Train accuracy: 0.780111, val accuracy: 0.692000\n",
      "Loss: 0.785563, Train accuracy: 0.798667, val accuracy: 0.688000\n",
      "Loss: 1.550324, Train accuracy: 0.811333, val accuracy: 0.692000\n",
      "Loss: 1.622988, Train accuracy: 0.751889, val accuracy: 0.667000\n",
      "Loss: 1.267998, Train accuracy: 0.818778, val accuracy: 0.701000\n",
      "Loss: 1.285565, Train accuracy: 0.804000, val accuracy: 0.698000\n",
      "Loss: 0.737236, Train accuracy: 0.800000, val accuracy: 0.676000\n",
      "learning_rate=0.01, reg_strength=0.001, n_layers=100\n",
      "Loss: 2.213749, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.094893, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.068557, Train accuracy: 0.263000, val accuracy: 0.258000\n",
      "Loss: 1.934277, Train accuracy: 0.306444, val accuracy: 0.316000\n",
      "Loss: 1.686588, Train accuracy: 0.392444, val accuracy: 0.373000\n",
      "Loss: 1.563164, Train accuracy: 0.470667, val accuracy: 0.463000\n",
      "Loss: 1.497146, Train accuracy: 0.538333, val accuracy: 0.523000\n",
      "Loss: 1.385728, Train accuracy: 0.571222, val accuracy: 0.573000\n",
      "Loss: 1.203647, Train accuracy: 0.614222, val accuracy: 0.603000\n",
      "Loss: 0.906315, Train accuracy: 0.643778, val accuracy: 0.631000\n",
      "Loss: 1.549811, Train accuracy: 0.654556, val accuracy: 0.647000\n",
      "Loss: 1.199189, Train accuracy: 0.687222, val accuracy: 0.675000\n",
      "Loss: 0.977139, Train accuracy: 0.701222, val accuracy: 0.689000\n",
      "Loss: 1.586269, Train accuracy: 0.703222, val accuracy: 0.676000\n",
      "Loss: 1.353734, Train accuracy: 0.707556, val accuracy: 0.680000\n",
      "Loss: 0.958558, Train accuracy: 0.722000, val accuracy: 0.694000\n",
      "Loss: 1.097104, Train accuracy: 0.728111, val accuracy: 0.694000\n",
      "Loss: 0.944513, Train accuracy: 0.745556, val accuracy: 0.715000\n",
      "Loss: 0.933745, Train accuracy: 0.753889, val accuracy: 0.721000\n",
      "Loss: 1.266439, Train accuracy: 0.751444, val accuracy: 0.699000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.956694, Train accuracy: 0.758778, val accuracy: 0.713000\n",
      "Loss: 0.732839, Train accuracy: 0.762000, val accuracy: 0.716000\n",
      "Loss: 0.800622, Train accuracy: 0.770111, val accuracy: 0.726000\n",
      "Loss: 1.101463, Train accuracy: 0.780889, val accuracy: 0.735000\n",
      "learning_rate=0.01, reg_strength=0.001, n_layers=200\n",
      "Loss: 2.230439, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228203, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238422, Train accuracy: 0.200333, val accuracy: 0.211000\n",
      "Loss: 1.875343, Train accuracy: 0.272778, val accuracy: 0.271000\n",
      "Loss: 2.122162, Train accuracy: 0.310778, val accuracy: 0.320000\n",
      "Loss: 1.819044, Train accuracy: 0.399000, val accuracy: 0.391000\n",
      "Loss: 1.770489, Train accuracy: 0.474444, val accuracy: 0.485000\n",
      "Loss: 1.390061, Train accuracy: 0.528222, val accuracy: 0.529000\n",
      "Loss: 1.582838, Train accuracy: 0.584667, val accuracy: 0.560000\n",
      "Loss: 1.252840, Train accuracy: 0.614889, val accuracy: 0.605000\n",
      "Loss: 1.067175, Train accuracy: 0.661889, val accuracy: 0.635000\n",
      "Loss: 1.183060, Train accuracy: 0.676000, val accuracy: 0.658000\n",
      "Loss: 0.892612, Train accuracy: 0.693444, val accuracy: 0.676000\n",
      "Loss: 1.034132, Train accuracy: 0.706444, val accuracy: 0.686000\n",
      "Loss: 1.269361, Train accuracy: 0.719444, val accuracy: 0.683000\n",
      "Loss: 0.802003, Train accuracy: 0.726333, val accuracy: 0.700000\n",
      "Loss: 1.091530, Train accuracy: 0.736444, val accuracy: 0.707000\n",
      "Loss: 1.221609, Train accuracy: 0.747667, val accuracy: 0.720000\n",
      "Loss: 1.178781, Train accuracy: 0.745444, val accuracy: 0.713000\n",
      "Loss: 0.835335, Train accuracy: 0.772222, val accuracy: 0.726000\n",
      "Loss: 0.810694, Train accuracy: 0.776556, val accuracy: 0.723000\n",
      "Loss: 0.785108, Train accuracy: 0.789222, val accuracy: 0.730000\n",
      "Loss: 0.761021, Train accuracy: 0.797222, val accuracy: 0.743000\n",
      "Loss: 0.924777, Train accuracy: 0.782778, val accuracy: 0.729000\n",
      "Loss: 1.122261, Train accuracy: 0.806222, val accuracy: 0.750000\n",
      "learning_rate=0.01, reg_strength=0.001, n_layers=500\n",
      "Loss: 2.235107, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255313, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172955, Train accuracy: 0.219333, val accuracy: 0.221000\n",
      "Loss: 1.860077, Train accuracy: 0.283222, val accuracy: 0.290000\n",
      "Loss: 1.855041, Train accuracy: 0.386222, val accuracy: 0.387000\n",
      "Loss: 1.505618, Train accuracy: 0.452000, val accuracy: 0.469000\n",
      "Loss: 1.520739, Train accuracy: 0.535889, val accuracy: 0.533000\n",
      "Loss: 1.450402, Train accuracy: 0.582667, val accuracy: 0.578000\n",
      "Loss: 1.218177, Train accuracy: 0.629111, val accuracy: 0.611000\n",
      "Loss: 1.146520, Train accuracy: 0.659222, val accuracy: 0.647000\n",
      "Loss: 1.285874, Train accuracy: 0.687000, val accuracy: 0.662000\n",
      "Loss: 1.018428, Train accuracy: 0.699778, val accuracy: 0.666000\n",
      "Loss: 1.055741, Train accuracy: 0.712889, val accuracy: 0.697000\n",
      "Loss: 1.240351, Train accuracy: 0.709333, val accuracy: 0.672000\n",
      "Loss: 1.051942, Train accuracy: 0.719444, val accuracy: 0.693000\n",
      "Loss: 0.742760, Train accuracy: 0.738000, val accuracy: 0.708000\n",
      "Loss: 0.972276, Train accuracy: 0.745889, val accuracy: 0.700000\n",
      "Loss: 0.893041, Train accuracy: 0.761556, val accuracy: 0.708000\n",
      "Loss: 0.928816, Train accuracy: 0.769778, val accuracy: 0.728000\n",
      "Loss: 0.702894, Train accuracy: 0.776000, val accuracy: 0.712000\n",
      "Loss: 0.715335, Train accuracy: 0.762556, val accuracy: 0.711000\n",
      "Loss: 0.699619, Train accuracy: 0.790000, val accuracy: 0.725000\n",
      "Loss: 0.725083, Train accuracy: 0.790111, val accuracy: 0.731000\n",
      "Loss: 1.091265, Train accuracy: 0.797889, val accuracy: 0.728000\n",
      "Loss: 0.756783, Train accuracy: 0.814778, val accuracy: 0.731000\n",
      "learning_rate=0.01, reg_strength=0.0001, n_layers=100\n",
      "Loss: 2.176837, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271658, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.140825, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.732347, Train accuracy: 0.254667, val accuracy: 0.256000\n",
      "Loss: 1.814228, Train accuracy: 0.309444, val accuracy: 0.316000\n",
      "Loss: 1.690495, Train accuracy: 0.388111, val accuracy: 0.391000\n",
      "Loss: 1.495006, Train accuracy: 0.462778, val accuracy: 0.460000\n",
      "Loss: 1.238107, Train accuracy: 0.540333, val accuracy: 0.521000\n",
      "Loss: 1.467603, Train accuracy: 0.574444, val accuracy: 0.570000\n",
      "Loss: 1.015147, Train accuracy: 0.632333, val accuracy: 0.622000\n",
      "Loss: 0.898040, Train accuracy: 0.648667, val accuracy: 0.623000\n",
      "Loss: 1.231975, Train accuracy: 0.671889, val accuracy: 0.651000\n",
      "Loss: 1.582058, Train accuracy: 0.687889, val accuracy: 0.675000\n",
      "Loss: 0.746982, Train accuracy: 0.699000, val accuracy: 0.679000\n",
      "Loss: 0.818858, Train accuracy: 0.711444, val accuracy: 0.685000\n",
      "Loss: 0.960710, Train accuracy: 0.724778, val accuracy: 0.700000\n",
      "Loss: 0.845446, Train accuracy: 0.736556, val accuracy: 0.703000\n",
      "Loss: 0.756786, Train accuracy: 0.757333, val accuracy: 0.701000\n",
      "Loss: 0.698226, Train accuracy: 0.755222, val accuracy: 0.713000\n",
      "Loss: 0.775099, Train accuracy: 0.761333, val accuracy: 0.702000\n",
      "Loss: 0.932073, Train accuracy: 0.775667, val accuracy: 0.708000\n",
      "Loss: 0.943913, Train accuracy: 0.787778, val accuracy: 0.713000\n",
      "Loss: 0.768056, Train accuracy: 0.798667, val accuracy: 0.738000\n",
      "Loss: 0.516573, Train accuracy: 0.788889, val accuracy: 0.717000\n",
      "Loss: 0.841449, Train accuracy: 0.803111, val accuracy: 0.730000\n",
      "learning_rate=0.01, reg_strength=0.0001, n_layers=200\n",
      "Loss: 2.179355, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.058406, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220259, Train accuracy: 0.201778, val accuracy: 0.209000\n",
      "Loss: 2.078077, Train accuracy: 0.272667, val accuracy: 0.265000\n",
      "Loss: 2.001652, Train accuracy: 0.337111, val accuracy: 0.340000\n",
      "Loss: 1.795178, Train accuracy: 0.412333, val accuracy: 0.407000\n",
      "Loss: 1.646462, Train accuracy: 0.513222, val accuracy: 0.498000\n",
      "Loss: 1.216715, Train accuracy: 0.571444, val accuracy: 0.564000\n",
      "Loss: 1.577962, Train accuracy: 0.620333, val accuracy: 0.606000\n",
      "Loss: 1.049976, Train accuracy: 0.653000, val accuracy: 0.641000\n",
      "Loss: 1.068186, Train accuracy: 0.675333, val accuracy: 0.660000\n",
      "Loss: 0.897150, Train accuracy: 0.676667, val accuracy: 0.657000\n",
      "Loss: 1.225898, Train accuracy: 0.691667, val accuracy: 0.667000\n",
      "Loss: 1.225084, Train accuracy: 0.720000, val accuracy: 0.689000\n",
      "Loss: 0.896058, Train accuracy: 0.727778, val accuracy: 0.697000\n",
      "Loss: 1.108369, Train accuracy: 0.729000, val accuracy: 0.701000\n",
      "Loss: 0.962522, Train accuracy: 0.758444, val accuracy: 0.723000\n",
      "Loss: 0.972177, Train accuracy: 0.760333, val accuracy: 0.715000\n",
      "Loss: 0.970885, Train accuracy: 0.774000, val accuracy: 0.727000\n",
      "Loss: 0.921097, Train accuracy: 0.787444, val accuracy: 0.727000\n",
      "Loss: 1.039315, Train accuracy: 0.784222, val accuracy: 0.730000\n",
      "Loss: 0.562887, Train accuracy: 0.793222, val accuracy: 0.732000\n",
      "Loss: 1.039355, Train accuracy: 0.811000, val accuracy: 0.746000\n",
      "Loss: 0.581704, Train accuracy: 0.810556, val accuracy: 0.732000\n",
      "Loss: 0.556796, Train accuracy: 0.816889, val accuracy: 0.736000\n",
      "learning_rate=0.01, reg_strength=0.0001, n_layers=500\n",
      "Loss: 2.229873, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.167281, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172464, Train accuracy: 0.236111, val accuracy: 0.240000\n",
      "Loss: 1.913722, Train accuracy: 0.300778, val accuracy: 0.307000\n",
      "Loss: 1.675919, Train accuracy: 0.388333, val accuracy: 0.380000\n",
      "Loss: 1.579893, Train accuracy: 0.480444, val accuracy: 0.473000\n",
      "Loss: 1.340461, Train accuracy: 0.554333, val accuracy: 0.544000\n",
      "Loss: 1.159640, Train accuracy: 0.611000, val accuracy: 0.589000\n",
      "Loss: 1.380793, Train accuracy: 0.640444, val accuracy: 0.646000\n",
      "Loss: 1.036835, Train accuracy: 0.655000, val accuracy: 0.647000\n",
      "Loss: 0.919955, Train accuracy: 0.689111, val accuracy: 0.658000\n",
      "Loss: 0.852818, Train accuracy: 0.700444, val accuracy: 0.684000\n",
      "Loss: 1.011998, Train accuracy: 0.712222, val accuracy: 0.685000\n",
      "Loss: 0.943587, Train accuracy: 0.737667, val accuracy: 0.710000\n",
      "Loss: 1.114814, Train accuracy: 0.743556, val accuracy: 0.712000\n",
      "Loss: 0.816646, Train accuracy: 0.749556, val accuracy: 0.714000\n",
      "Loss: 0.883774, Train accuracy: 0.766111, val accuracy: 0.709000\n",
      "Loss: 0.779357, Train accuracy: 0.780889, val accuracy: 0.729000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.724624, Train accuracy: 0.779778, val accuracy: 0.717000\n",
      "Loss: 0.498387, Train accuracy: 0.796333, val accuracy: 0.722000\n",
      "Loss: 0.785125, Train accuracy: 0.803556, val accuracy: 0.722000\n",
      "Loss: 0.648727, Train accuracy: 0.808444, val accuracy: 0.735000\n",
      "Loss: 0.593744, Train accuracy: 0.800333, val accuracy: 0.724000\n",
      "Loss: 0.785119, Train accuracy: 0.818556, val accuracy: 0.742000\n",
      "Loss: 0.600855, Train accuracy: 0.838333, val accuracy: 0.747000\n",
      "learning_rate=0.01, reg_strength=1e-05, n_layers=100\n",
      "Loss: 2.167661, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243200, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.055823, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.160959, Train accuracy: 0.274889, val accuracy: 0.269000\n",
      "Loss: 1.988890, Train accuracy: 0.304111, val accuracy: 0.299000\n",
      "Loss: 1.836951, Train accuracy: 0.398889, val accuracy: 0.391000\n",
      "Loss: 1.462867, Train accuracy: 0.469556, val accuracy: 0.468000\n",
      "Loss: 1.377140, Train accuracy: 0.539889, val accuracy: 0.535000\n",
      "Loss: 1.331710, Train accuracy: 0.581000, val accuracy: 0.569000\n",
      "Loss: 1.371275, Train accuracy: 0.615889, val accuracy: 0.598000\n",
      "Loss: 1.226457, Train accuracy: 0.652444, val accuracy: 0.643000\n",
      "Loss: 1.199859, Train accuracy: 0.672111, val accuracy: 0.660000\n",
      "Loss: 1.094355, Train accuracy: 0.696444, val accuracy: 0.672000\n",
      "Loss: 1.152620, Train accuracy: 0.700333, val accuracy: 0.682000\n",
      "Loss: 1.067098, Train accuracy: 0.724222, val accuracy: 0.694000\n",
      "Loss: 1.059024, Train accuracy: 0.724889, val accuracy: 0.696000\n",
      "Loss: 0.994985, Train accuracy: 0.743444, val accuracy: 0.697000\n",
      "Loss: 0.860095, Train accuracy: 0.753889, val accuracy: 0.712000\n",
      "Loss: 0.710266, Train accuracy: 0.765556, val accuracy: 0.727000\n",
      "Loss: 0.565228, Train accuracy: 0.760444, val accuracy: 0.712000\n",
      "Loss: 0.908396, Train accuracy: 0.763444, val accuracy: 0.714000\n",
      "Loss: 0.865163, Train accuracy: 0.793889, val accuracy: 0.737000\n",
      "Loss: 0.913819, Train accuracy: 0.778778, val accuracy: 0.715000\n",
      "Loss: 0.558922, Train accuracy: 0.804222, val accuracy: 0.728000\n",
      "Loss: 0.720301, Train accuracy: 0.803778, val accuracy: 0.732000\n",
      "learning_rate=0.01, reg_strength=1e-05, n_layers=200\n",
      "Loss: 2.247106, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.119029, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236551, Train accuracy: 0.218222, val accuracy: 0.228000\n",
      "Loss: 2.015893, Train accuracy: 0.277333, val accuracy: 0.280000\n",
      "Loss: 1.894738, Train accuracy: 0.349333, val accuracy: 0.344000\n",
      "Loss: 1.505452, Train accuracy: 0.441333, val accuracy: 0.432000\n",
      "Loss: 1.352841, Train accuracy: 0.512889, val accuracy: 0.524000\n",
      "Loss: 1.334339, Train accuracy: 0.571111, val accuracy: 0.557000\n",
      "Loss: 0.890823, Train accuracy: 0.619000, val accuracy: 0.594000\n",
      "Loss: 1.209967, Train accuracy: 0.645000, val accuracy: 0.621000\n",
      "Loss: 0.936986, Train accuracy: 0.677444, val accuracy: 0.659000\n",
      "Loss: 1.126510, Train accuracy: 0.684778, val accuracy: 0.671000\n",
      "Loss: 0.858896, Train accuracy: 0.711000, val accuracy: 0.685000\n",
      "Loss: 0.926687, Train accuracy: 0.718000, val accuracy: 0.692000\n",
      "Loss: 0.801645, Train accuracy: 0.736889, val accuracy: 0.709000\n",
      "Loss: 0.923811, Train accuracy: 0.744000, val accuracy: 0.723000\n",
      "Loss: 0.886387, Train accuracy: 0.751111, val accuracy: 0.701000\n",
      "Loss: 0.656219, Train accuracy: 0.754333, val accuracy: 0.708000\n",
      "Loss: 1.129340, Train accuracy: 0.768111, val accuracy: 0.712000\n",
      "Loss: 0.713159, Train accuracy: 0.764556, val accuracy: 0.709000\n",
      "Loss: 0.489978, Train accuracy: 0.777444, val accuracy: 0.727000\n",
      "Loss: 0.810371, Train accuracy: 0.805556, val accuracy: 0.729000\n",
      "Loss: 0.792239, Train accuracy: 0.807667, val accuracy: 0.730000\n",
      "Loss: 0.685171, Train accuracy: 0.815778, val accuracy: 0.727000\n",
      "Loss: 0.610085, Train accuracy: 0.826222, val accuracy: 0.747000\n",
      "learning_rate=0.01, reg_strength=1e-05, n_layers=500\n",
      "Loss: 2.199300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.041324, Train accuracy: 0.245556, val accuracy: 0.248000\n",
      "Loss: 1.907209, Train accuracy: 0.302667, val accuracy: 0.312000\n",
      "Loss: 1.875955, Train accuracy: 0.399333, val accuracy: 0.393000\n",
      "Loss: 1.424115, Train accuracy: 0.490444, val accuracy: 0.501000\n",
      "Loss: 1.318217, Train accuracy: 0.555000, val accuracy: 0.550000\n",
      "Loss: 1.431573, Train accuracy: 0.609778, val accuracy: 0.588000\n",
      "Loss: 1.094578, Train accuracy: 0.642667, val accuracy: 0.621000\n",
      "Loss: 1.143519, Train accuracy: 0.669778, val accuracy: 0.663000\n",
      "Loss: 1.092784, Train accuracy: 0.698333, val accuracy: 0.683000\n",
      "Loss: 0.950956, Train accuracy: 0.696333, val accuracy: 0.672000\n",
      "Loss: 1.227798, Train accuracy: 0.707778, val accuracy: 0.686000\n",
      "Loss: 1.432719, Train accuracy: 0.731222, val accuracy: 0.709000\n",
      "Loss: 1.058080, Train accuracy: 0.740222, val accuracy: 0.703000\n",
      "Loss: 0.855155, Train accuracy: 0.751667, val accuracy: 0.715000\n",
      "Loss: 0.702224, Train accuracy: 0.774111, val accuracy: 0.711000\n",
      "Loss: 1.407988, Train accuracy: 0.776111, val accuracy: 0.712000\n",
      "Loss: 0.766261, Train accuracy: 0.770889, val accuracy: 0.705000\n",
      "Loss: 0.536929, Train accuracy: 0.790889, val accuracy: 0.721000\n",
      "Loss: 0.744517, Train accuracy: 0.806111, val accuracy: 0.724000\n",
      "Loss: 0.814035, Train accuracy: 0.805111, val accuracy: 0.724000\n",
      "Loss: 0.767407, Train accuracy: 0.824444, val accuracy: 0.729000\n",
      "Loss: 0.692792, Train accuracy: 0.824000, val accuracy: 0.730000\n",
      "Loss: 0.474768, Train accuracy: 0.848444, val accuracy: 0.741000\n",
      "learning_rate=0.001, reg_strength=0.001, n_layers=100\n",
      "Loss: 2.271356, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.319671, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263415, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282778, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.171913, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266590, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.209124, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.152558, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206440, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.210315, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214136, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214271, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213783, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234545, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220472, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.169877, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280701, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213719, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.121134, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272879, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238315, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.194720, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.001, n_layers=200\n",
      "Loss: 2.288853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269675, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239145, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240711, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255192, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.199716, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.197902, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265036, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279921, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.248133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.162478, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.142007, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173361, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290107, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.162645, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.166422, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.329445, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.165950, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177967, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.193385, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.219649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251937, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182253, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.125165, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.001, n_layers=500\n",
      "Loss: 2.280122, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265037, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251987, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252235, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249862, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259937, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263843, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299628, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.215323, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223136, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250252, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269812, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.118857, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288840, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.200989, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237983, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236568, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.189694, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.161584, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.142981, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.099586, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279371, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.165312, Train accuracy: 0.196889, val accuracy: 0.206000\n",
      "Loss: 1.974718, Train accuracy: 0.205000, val accuracy: 0.215000\n",
      "learning_rate=0.001, reg_strength=0.0001, n_layers=100\n",
      "Loss: 2.298077, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259166, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275917, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.204277, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216177, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251118, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.362154, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242551, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196417, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300409, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.368397, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273986, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241175, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302699, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.116394, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245055, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.180778, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224298, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273515, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309396, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223697, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196258, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301106, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.154722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.176901, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.0001, n_layers=200\n",
      "Loss: 2.304669, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267608, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243304, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247693, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.144526, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218771, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217204, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.113540, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332163, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168266, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.160621, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242374, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.210012, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.130492, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268781, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.161165, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.193664, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.096700, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.072399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.170500, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258519, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.079857, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.125933, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.083136, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.0001, n_layers=500\n",
      "Loss: 2.285201, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289374, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247574, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245040, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212589, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257428, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.188807, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279942, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224345, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192073, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236695, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157062, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262661, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256041, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.155272, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.144288, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.193546, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173961, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.160365, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262729, Train accuracy: 0.198333, val accuracy: 0.209000\n",
      "Loss: 2.111563, Train accuracy: 0.207444, val accuracy: 0.214000\n",
      "learning_rate=0.001, reg_strength=1e-05, n_layers=100\n",
      "Loss: 2.303567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273429, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246577, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238876, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237888, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244474, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220982, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.165570, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207734, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173895, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.134821, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.140661, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.134639, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253305, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.134153, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.181034, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302595, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251030, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308970, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.100980, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.232950, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298393, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196989, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=1e-05, n_layers=200\n",
      "Loss: 2.294350, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260322, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214531, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226487, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182917, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265923, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216406, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-976826131721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data/CSC/DL/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data/CSC/DL/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# After that, implement l2 regularization on all params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/data/CSC/DL/assignment2/layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, d_out)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# the previous assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3]\n",
    "reg_strength = [1e-3, 1e-4, 1e-5]\n",
    "learning_rate_decay = 0.9\n",
    "hidden_layer_size = [100, 200, 500]\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "for lr, reg, n_layers in itertools.product(learning_rates, \n",
    "                                           reg_strength,\n",
    "                                           hidden_layer_size):\n",
    "    print(f'learning_rate={lr}, reg_strength={reg}, n_layers={n_layers}')\n",
    "    \n",
    "    model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=n_layers, reg=reg)\n",
    "    dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=lr, num_epochs=num_epochs, batch_size=batch_size)\n",
    "    loss_hist, train_hist, val_hist = trainer.fit()\n",
    "    accuracy = val_hist[-1]\n",
    "    if not best_val_accuracy or accuracy > best_val_accuracy:\n",
    "        best_classifier = model\n",
    "        best_val_accuracy = accuracy\n",
    "        loss_history = loss_hist\n",
    "        train_history = train_hist\n",
    "        val_history = val_hist\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd23aecc3c8>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.707000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
