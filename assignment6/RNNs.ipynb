{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CEQYrsVt0-_",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "outputId": "f7126bf4-77ef-44dc-a208-615847ff1e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5MB 23kB/s \n",
            "\u001b[31mERROR: fastai 1.0.52 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0MB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 2.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "810b3561-f251-4161-b363-a20a2bc3114b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "f24520f6-5ab0-4246-b3da-7713e9c0cb2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "a38382ab-4c4b-4b48-f434-16157ae64f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "4b48669d-6117-4c26-d5b2-430fbce44cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'ADJ', '.', 'VERB', 'ADV', 'DET', 'ADP', 'CONJ', 'X', 'NUM', 'PRON', 'NOUN', 'PRT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "0d5343d5-2131-4d5c-b8d1-bdb9e387c737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEyCAYAAABH+Yw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHWVJREFUeJzt3X20XXV95/H3p2Fw0QcKSkopoEEN\nWqAaJUtZrXZQRAPtEuyimkwr0TJGl7A6UKcjtp2FU3UGbRlmMVVcWDJARwlUa8m4YjFFrO1MUYKk\nPCkQEEsyPKSAMh0cEfzOH+d3ded6k9zce3Pv73rfr7XOumd/9/7t8z2bfS+f7IdzUlVIkiSpXz82\n1w1IkiRp1wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLn\n9pnrBmbaQQcdVEuWLJnrNiRJknbrpptu+qeqWry75X7kAtuSJUvYtGnTXLchSZK0W0m+MZnlPCUq\nSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktS53Qa2\nJGuTPJzktkHtqiSb2+O+JJtbfUmSbw/mfXQw5tgktybZkuSiJGn1ZybZmOTu9vPAVk9bbkuSW5K8\ndObfviRJUv8mc4TtMmDFsFBVb6qqZVW1DPgU8BeD2feMzauqdwzqFwNvA5a2x9g6zwWuq6qlwHVt\nGuCkwbJr2nhJkqQFZ7ffJVpVX0yyZKJ57SjZG4FX72odSQ4B9q+qG9r0FcCpwGeBU4Dj26KXA18A\n3t3qV1RVATckOSDJIVX1wG7flSRJmlEXbrxrWuPPOfHIGepkYZruNWyvBB6qqrsHtSOS3Jzkb5K8\nstUOBbYOltnaagAHD0LYg8DBgzH372TMDpKsSbIpyabt27dP4+1IkiT1Z7qBbRVw5WD6AeDZVfUS\n4HeATyTZf7Ira0fTak+bqKpLqmp5VS1fvHjxng6XJEnq2m5Pie5Mkn2AXwOOHatV1XeA77TnNyW5\nBzgS2AYcNhh+WKsBPDR2qrOdOn241bcBh+9kjCRJ0oIxnSNsrwG+VlXfP9WZZHGSRe35cxndMHBv\nO+X5eJLj2nVvpwPXtGHrgdXt+epx9dPb3aLHAd/y+jVJkrQQTeZjPa4E/h54QZKtSc5os1ay4+lQ\ngF8Gbmkf8/FJ4B1V9Wib907gT4EtwD2MbjgAOB84McndjELg+a2+Abi3Lf+xNl6SJGnBmcxdoqt2\nUn/LBLVPMfqYj4mW3wQcM0H9EeCECeoFnLm7/iRJkn7U+U0HkiRJnTOwSZIkdc7AJkmS1DkDmyRJ\nUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJ\nnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1\nzsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1LndBrYka5M8nOS2Qe29SbYl2dweJw/mvSfJ\nliR3JnndoL6i1bYkOXdQPyLJl1r9qiT7tvoz2vSWNn/JTL1pSZKk+WQyR9guA1ZMUL+wqpa1xwaA\nJEcBK4Gj25iPJFmUZBHwYeAk4ChgVVsW4INtXc8HHgPOaPUzgMda/cK2nCRJ0oKz28BWVV8EHp3k\n+k4B1lXVd6rq68AW4GXtsaWq7q2qJ4F1wClJArwa+GQbfzlw6mBdl7fnnwROaMtLkiQtKNO5hu2s\nJLe0U6YHttqhwP2DZba22s7qzwK+WVVPjavvsK42/1tteUmSpAVlqoHtYuB5wDLgAeCCGetoCpKs\nSbIpyabt27fPZSuSJEkzbkqBraoeqqqnq+p7wMcYnfIE2AYcPlj0sFbbWf0R4IAk+4yr77CuNv+n\n2/IT9XNJVS2vquWLFy+eyluSJEnq1pQCW5JDBpNvAMbuIF0PrGx3eB4BLAW+DNwILG13hO7L6MaE\n9VVVwPXAaW38auCawbpWt+enAZ9vy0uSJC0o++xugSRXAscDByXZCpwHHJ9kGVDAfcDbAarq9iRX\nA3cATwFnVtXTbT1nAdcCi4C1VXV7e4l3A+uSvB+4Gbi01S8F/izJFkY3Payc9ruVJEmah3Yb2Kpq\n1QTlSyeojS3/AeADE9Q3ABsmqN/LD06pDuv/D/j13fUnSZL0o85vOpAkSeqcgU2SJKlzBjZJkqTO\nGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjpn\nYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyB\nTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6t9vAlmRtkoeT3Dao/VGS\nryW5JcmnkxzQ6kuSfDvJ5vb46GDMsUluTbIlyUVJ0urPTLIxyd3t54GtnrbclvY6L535ty9JktS/\nyRxhuwxYMa62ETimql4E3AW8ZzDvnqpa1h7vGNQvBt4GLG2PsXWeC1xXVUuB69o0wEmDZde08ZIk\nSQvObgNbVX0ReHRc7XNV9VSbvAE4bFfrSHIIsH9V3VBVBVwBnNpmnwJc3p5fPq5+RY3cABzQ1iNJ\nkrSgzMQ1bL8FfHYwfUSSm5P8TZJXttqhwNbBMltbDeDgqnqgPX8QOHgw5v6djJEkSVow9pnO4CS/\nDzwFfLyVHgCeXVWPJDkW+MskR092fVVVSWoKfaxhdNqUZz/72Xs6XJIkqWtTPsKW5C3ArwK/0U5z\nUlXfqapH2vObgHuAI4Ft7Hja9LBWA3ho7FRn+/lwq28DDt/JmB1U1SVVtbyqli9evHiqb0mSJKlL\nUwpsSVYA/w54fVU9MagvTrKoPX8uoxsG7m2nPB9Pcly7O/R04Jo2bD2wuj1fPa5+ertb9DjgW4NT\np5IkSQvGbk+JJrkSOB44KMlW4DxGd4U+A9jYPp3jhnZH6C8Df5jku8D3gHdU1dgNC+9kdMfpfoyu\neRu77u184OokZwDfAN7Y6huAk4EtwBPAW6fzRiVJkuar3Qa2qlo1QfnSnSz7KeBTO5m3CThmgvoj\nwAkT1As4c3f9SZIk/ajzmw4kSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIk\nqXPT+i5RaW+6cONdUx57zolHzmAnkiTNLY+wSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXO\nwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkD\nmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdm1RgS7I2ycNJbhvUnplkY5K7288D\nWz1JLkqyJcktSV46GLO6LX93ktWD+rFJbm1jLkqSXb2GJEnSQjLZI2yXASvG1c4FrquqpcB1bRrg\nJGBpe6wBLoZR+ALOA14OvAw4bxDALgbeNhi3YjevIUmStGBMKrBV1ReBR8eVTwEub88vB04d1K+o\nkRuAA5IcArwO2FhVj1bVY8BGYEWbt39V3VBVBVwxbl0TvYYkSdKCMZ1r2A6uqgfa8weBg9vzQ4H7\nB8ttbbVd1bdOUN/Va+wgyZokm5Js2r59+xTfjiRJUp9m5KaDdmSsZmJdU3mNqrqkqpZX1fLFixfv\nzTYkSZJm3XQC20PtdCbt58Otvg04fLDcYa22q/phE9R39RqSJEkLxnQC23pg7E7P1cA1g/rp7W7R\n44BvtdOa1wKvTXJgu9ngtcC1bd7jSY5rd4eePm5dE72GJEnSgrHPZBZKciVwPHBQkq2M7vY8H7g6\nyRnAN4A3tsU3ACcDW4AngLcCVNWjSd4H3NiW+8OqGruR4Z2M7kTdD/hse7CL15AkSVowJhXYqmrV\nTmadMMGyBZy5k/WsBdZOUN8EHDNB/ZGJXkOSJGkh8ZsOJEmSOmdgkyRJ6pyBTZIkqXOTuoZN89+F\nG++a1vhzTjxyhjqRJEl7yiNskiRJnTOwSZIkdc5TopLmFU/vS1qIPMImSZLUOQObJElS5wxskiRJ\nnTOwSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ3zc9ikBczPNJOk+cEjbJIkSZ0zsEmSJHXOwCZJktQ5\nA5skSVLnDGySJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJUucM\nbJIkSZ2bcmBL8oIkmwePx5OcneS9SbYN6icPxrwnyZYkdyZ53aC+otW2JDl3UD8iyZda/aok+079\nrUqSJM1PUw5sVXVnVS2rqmXAscATwKfb7AvH5lXVBoAkRwErgaOBFcBHkixKsgj4MHAScBSwqi0L\n8MG2rucDjwFnTLVfSZKk+WqmTomeANxTVd/YxTKnAOuq6jtV9XVgC/Cy9thSVfdW1ZPAOuCUJAFe\nDXyyjb8cOHWG+pUkSZo3ZiqwrQSuHEyfleSWJGuTHNhqhwL3D5bZ2mo7qz8L+GZVPTWu/kOSrEmy\nKcmm7du3T//dSJIkdWTaga1dV/Z64M9b6WLgecAy4AHggum+xu5U1SVVtbyqli9evHhvv5wkSdKs\n2mcG1nES8JWqeghg7CdAko8Bn2mT24DDB+MOazV2Un8EOCDJPu0o23B5SZKkBWMmTomuYnA6NMkh\ng3lvAG5rz9cDK5M8I8kRwFLgy8CNwNJ2R+i+jE6vrq+qAq4HTmvjVwPXzEC/kiRJ88q0jrAl+Qng\nRODtg/KHkiwDCrhvbF5V3Z7kauAO4CngzKp6uq3nLOBaYBGwtqpub+t6N7AuyfuBm4FLp9OvJEnS\nfDStwFZV/5fRzQHD2pt3sfwHgA9MUN8AbJigfi+ju0glSZIWLL/pQJIkqXMGNkmSpM4Z2CRJkjpn\nYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyB\nTZIkqXMGNkmSpM4Z2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpcwY2\nSZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM7tM9cNzEcXbrxrWuPPOfHIGepEkiQtBB5hkyRJ\n6ty0A1uS+5LcmmRzkk2t9swkG5Pc3X4e2OpJclGSLUluSfLSwXpWt+XvTrJ6UD+2rX9LG5vp9ixJ\nkjSfzNQRtldV1bKqWt6mzwWuq6qlwHVtGuAkYGl7rAEuhlHAA84DXg68DDhvLOS1Zd42GLdihnqW\nJEmaF/bWKdFTgMvb88uBUwf1K2rkBuCAJIcArwM2VtWjVfUYsBFY0ebtX1U3VFUBVwzWJUmStCDM\nRGAr4HNJbkqyptUOrqoH2vMHgYPb80OB+wdjt7barupbJ6jvIMmaJJuSbNq+fft0348kSVJXZuIu\n0VdU1bYkPwNsTPK14cyqqiQ1A6+zU1V1CXAJwPLly/fqa0mSJM22aR9hq6pt7efDwKcZXYP2UDud\nSfv5cFt8G3D4YPhhrbar+mET1CVJkhaMaQW2JD+R5KfGngOvBW4D1gNjd3quBq5pz9cDp7e7RY8D\nvtVOnV4LvDbJge1mg9cC17Z5jyc5rt0devpgXZIkSQvCdE+JHgx8un3Sxj7AJ6rqr5LcCFyd5Azg\nG8Ab2/IbgJOBLcATwFsBqurRJO8DbmzL/WFVPdqevxO4DNgP+Gx7SJIkLRjTCmxVdS/w4gnqjwAn\nTFAv4MydrGstsHaC+ibgmOn0KUmSNJ/5TQeSJEmdM7BJkiR1zsAmSZLUOQObJElS5wxskiRJnTOw\nSZIkdc7AJkmS1DkDmyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGySJEmdM7BJkiR1zsAm\nSZLUuX3mugHpR8mFG++a8thzTjxyBjuRJP0o8QibJElS5wxskiRJnTOwSZIkdc7AJkmS1DkDmyRJ\nUucMbJIkSZ3zYz0kSZoDfgyQ9oRH2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTOGdgkSZI6N+XAluTw\nJNcnuSPJ7Un+Tau/N8m2JJvb4+TBmPck2ZLkziSvG9RXtNqWJOcO6kck+VKrX5Vk36n2K0mSNF9N\n5wjbU8C7quoo4DjgzCRHtXkXVtWy9tgA0OatBI4GVgAfSbIoySLgw8BJwFHAqsF6PtjW9XzgMeCM\nafQrSZI0L005sFXVA1X1lfb8/wBfBQ7dxZBTgHVV9Z2q+jqwBXhZe2ypqnur6klgHXBKkgCvBj7Z\nxl8OnDrVfiVJkuarGbmGLckS4CXAl1rprCS3JFmb5MBWOxS4fzBsa6vtrP4s4JtV9dS4+kSvvybJ\npiSbtm/fPgPvSJIkqR/T/qaDJD8JfAo4u6oeT3Ix8D6g2s8LgN+a7uvsSlVdAlwCsHz58tqbryVJ\ne8pPtJc0XdMKbEn+BaOw9vGq+guAqnpoMP9jwGfa5Dbg8MHww1qNndQfAQ5Isk87yjZcXpIkacGY\nzl2iAS4FvlpV/3lQP2Sw2BuA29rz9cDKJM9IcgSwFPgycCOwtN0Rui+jGxPWV1UB1wOntfGrgWum\n2q8kSdJ8NZ0jbL8EvBm4NcnmVvs9Rnd5LmN0SvQ+4O0AVXV7kquBOxjdYXpmVT0NkOQs4FpgEbC2\nqm5v63s3sC7J+4GbGQVESZKkBWXKga2q/g7IBLM27GLMB4APTFDfMNG4qrqX0V2kkiRJC5bfdCBJ\nktQ5A5skSVLnDGySJEmdm/bnsEmSJPVoOp+BCH19DqJH2CRJkjpnYJMkSeqcgU2SJKlzBjZJkqTO\nGdgkSZI6Z2CTJEnqnIFNkiSpcwY2SZKkzhnYJEmSOmdgkyRJ6pyBTZIkqXMGNkmSpM4Z2CRJkjq3\nz1w3IEnSdF248a5pjT/nxCNnqBNp7/AImyRJUucMbJIkSZ0zsEmSJHXOwCZJktQ5A5skSVLnDGyS\nJEmdM7BJkiR1zsAmSZLUOQObJElS57oPbElWJLkzyZYk5851P5IkSbOt68CWZBHwYeAk4ChgVZKj\n5rYrSZKk2dV1YANeBmypqnur6klgHXDKHPckSZI0q3r/8vdDgfsH01uBl89RL5K0IPhF6lJ/UlVz\n3cNOJTkNWFFV/7pNvxl4eVWdNW65NcCaNvkC4M5ZbfSHHQT80xz3sKfsee+bb/2CPc+G+dYv2PNs\nmW89z7d+oY+en1NVi3e3UO9H2LYBhw+mD2u1HVTVJcAls9XU7iTZVFXL57qPPWHPe9986xfseTbM\nt37BnmfLfOt5vvUL86vn3q9huxFYmuSIJPsCK4H1c9yTJEnSrOr6CFtVPZXkLOBaYBGwtqpun+O2\nJEmSZlXXgQ2gqjYAG+a6jz3UzenZPWDPe9986xfseTbMt37BnmfLfOt5vvUL86jnrm86kCRJUv/X\nsEmSJC14BjZJkqTOGdimIcmpSSrJC9v0kiTfTnJzkq8m+XKStwyWf0uSP5mzhjuV5PokrxtXOzvJ\nZ9v23Dx4nN7m35fk1iS3JPmbJM8ZjH26LfsPSb6S5Bdn6X1Men9o87Ym+bFx69icZFY+HHqwnW5v\n2+pdY/0kOT7Jt8Zt+zcNnj+YZNtget9Z6nkqv3PbW493JHnbLPX5s0nWJbknyU1JNiQ5MsnRST7f\nvh/57iT/PkkGvX4vyYsG67ktyZL2/L4kB81G/xNJcniSryd5Zps+sE0vmcOeKskFg+l/m+S97fll\n7bM8h8v/c/u5pI19/2DeQUm+uzf+Rg9+125L8udJfnyC+v9IcsBgzJT3lRnod6fbtU2vSfK19vhy\nklcM5u2wn7a/JZ+Zjb4neB+T3u5JfmHw9+zRtm9vTvLXe6O3qTCwTc8q4O/azzH3VNVLqurnGX0M\nydlJ3jon3c0fVzLaVkMrgf/EaHsuGzyuGCzzqqp6EfAF4A8G9W+3ZV8MvKetZzZMen+oqvuAfwRe\nObZgCyE/VVVfmqV+x7bT0cCJjL6z97zB/L8dt+2vGnsOfBS4cDDvyVnqeSq/c1e1no8H/mOSg/dm\ng+1/qp8GvlBVz6uqYxnthwcz+lii86vqBcCLgV8E3jkYvhX4/b3Z31RV1f3AxcD5rXQ+cEnbl+fK\nd4Bfm2KQ/TrwK4PpXwf21qcQjP2uHQM8CbxjgvqjwJkASfZjbveVnW7XJL8KvB14RVW9sL2XTyT5\n2Umuezb38Ulv96q6dfD3bT3wu236NbPU624Z2KYoyU8CrwDO4IfDBgBVdS/wO8Bvz2Jr89EngV8Z\nO0rT/rX1c+z4tWS78veMvsZsIvsDj02zv92a4v4wPqiuZPR9ubOuqh5m9G0hZ439K7430/2da+/x\nHuA54+fNsFcB362qjw5e+x+AI4H/WVWfa7UngLOAcwdjPwMcneQFe7nHqboQOC7J2Yz+W/zxHPfz\nFKO7/M6ZwtgngK8mGfvQ1DcBV89UY7vwt8DzJ6gP/479K+Z2X9nVdn03ozDzT623rwCX08LmJMzV\nPj6Z7d41A9vUnQL8VVXdBTyS5NidLPcV4IWz19b8U1WPAl9mdIQHRv8zvhoo4HnZ8bTcKydYxQrg\nLwfT+7Vlvwb8KfC+vdj+mKnsD1cDpyYZ+3idNzEKcXOihZ1FwM+00ivHbfvnzVVvzbR+55I8F3gu\nsGXvtQjAMcBNE9SPHl+vqnuAn0yyfyt9D/gQ8Ht7tcMpqqrvAr/LKLid3abn2oeB30jy01MYuw5Y\nmeRw4Gngf89oZ+O03/WTgFvH1RcBJ/CDD4bvYV/Z2Xb9od6ATa0+GbO+j+/Bdu+agW3qVvGDoyHr\n2PEUzVCXRys6NDzatJIfBJfxp0T/djDm+iTbGP0iDoPO2OHuFzIKc1fMwlGjPd4fquoh4DbghCTL\ngKeq6ra92uWeGX9K9J457meqv3NvSrKZ0T7y9vYPhJ59gtFRrCPmupGdOAl4gFEwnXNV9ThwBT98\nVHWiz6waX/srRpcDrASumvnuvm+/tg9uYnQpxKXj6g8yOmW+cQ/Xu9f2lV1s190OnURttvbxvbXd\n50T3H5zbo4wuun018AtJitFRiWL0L5LxXgJ8dRbbm6+uAS5M8lLgx6vqpklciPoq4JvAx4H/wOhU\n2A6q6u/bdRiLgYdntONmmvvDWFB9iDk8ugbfPwL1NKPt9PNz2ct409zGV1XVWXu/y++7HThtgvod\nwC8PC22b/3NVPT72b4r2DS8XMDr11JX2D4sTgeOAv0uyrqoemOO2AP4LoyOr/21QewQ4cGyi7UM7\nfMl3VT2Z5CbgXcBRwOv3Un/fbtdGTVhvF8Nfy+i04kX0s69MtF3vAI4FPj+oHcsPrv8b2+5j23qi\n7T5b+/iebveueYRtak4D/qyqnlNVS6rqcEYXsA6/qH7sWqw/Bv7rrHc4z1TVPwPXA2vZg+BSVU8B\nZwOntz/IO2gX8i9i9Edkb5nO/vAXwMmMTofOyfVrAEkWM7qR4E+qz0/Tnk+/c58HnpFkzVih3RV3\nJ/CKJK9ptf0Y/U/iQxOs4zLgNYz+odGFdpT6YkanQv8R+CPm/ho24PuXVVzN6PrGMV9gdHR17A7m\ntzD6GzPeBcC75/LIa7tG7beBd7XTdx+ng31lJ9v1Q8AHkzyr9baM0bb9SJv/BeDNbd4i4DeZeLvv\ntb4na4Lt3jUD29SsYnQX2NCnGN0J9ry0jxhgtKNfVFVj/zrZh9HdN13J6CMHfm6u+2AU1F7MjoFt\n/DVsE11M/kAbM3bR69g1bJsZneZYXVVP78W+p7o/UFXfZHTR60PtGrLZNLadbgf+GvgcoyOVY8Zf\nwzbRUaPZMuVtPNta4H0D8JqMPtbjdkZ3Kj/I6Dq8P0hyJ6PraW4EfuhjJNpdtxfxg+sJYe7/frwN\n+MeqGjt99BHg55P8yznsaegC4Pt3NVbVZxhdaH5T+1vwS0xwRKeqbq+qy2ety52oqpuBW4BVVfVt\nprevzKTx23U9o39Y/692nfDHgN8cHGl9H/D8JP8A3MzomtH/Pgd9T8pwu89lH5PhV1PNoiQXAndX\n1Ud2u7AkNe0I6Oaqmhd3s0maeR5hmyVJPgu8iNGhbkmalCSvZ3Sk6D1z3YukueMRNkmSpM55hE2S\nJKlzBjZJkqTOGdgkSZI6Z2CTJEnqnIFNkiSpc/8fRwR85Eb+IsQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "1268fd67-5baa-414c-db70-e26401faae96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "bd7611a4-23df-4b78-ae48-028d1a7fb101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "149330a3-29a6-4220-efe4-54d60efbbc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "630da86c-d358-4274-d700-a438bf573cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        out, _ = self._lstm(emb)\n",
        "        return self._out_layer(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "ac7ada73-5e6d-4945-b9f0-0114b88967f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "mask = (y_batch != 0).float()\n",
        "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "total_count = mask.sum().item()\n",
        "\n",
        "correct_count / total_count"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs:  torch.Size([32, 4])\n",
            "Emb:  torch.Size([32, 4, 100])\n",
            "Output of LSTM:  torch.Size([32, 4, 128])\n",
            "Final output torch.Size([32, 4, 13])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.043478260869565216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "aec80ea4-e002-49ff-f814-a306ab276295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.transpose(2, 1), y_batch)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5568, grad_fn=<NllLoss2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "                mask = (y_batch != 0).float()\n",
        "                cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "                cur_sum_count = mask.sum().item()\n",
        "                \n",
        "                # cur_correct_count, cur_sum_count = <calc accuracy>\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "73de4160-07e1-4d22-de0f-7820d13cf324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.31695, Accuracy = 72.10%: 100%|██████████| 572/572 [00:13<00:00, 41.02it/s]\n",
            "[1 / 50]   Val: Loss = 0.10835, Accuracy = 84.37%: 100%|██████████| 13/13 [00:00<00:00, 37.60it/s]\n",
            "[2 / 50] Train: Loss = 0.10138, Accuracy = 89.89%: 100%|██████████| 572/572 [00:13<00:00, 40.96it/s]\n",
            "[2 / 50]   Val: Loss = 0.07997, Accuracy = 88.98%: 100%|██████████| 13/13 [00:00<00:00, 38.25it/s]\n",
            "[3 / 50] Train: Loss = 0.06761, Accuracy = 93.25%: 100%|██████████| 572/572 [00:14<00:00, 40.76it/s]\n",
            "[3 / 50]   Val: Loss = 0.07459, Accuracy = 90.74%: 100%|██████████| 13/13 [00:00<00:00, 39.54it/s]\n",
            "[4 / 50] Train: Loss = 0.05123, Accuracy = 94.82%: 100%|██████████| 572/572 [00:13<00:00, 40.96it/s]\n",
            "[4 / 50]   Val: Loss = 0.06969, Accuracy = 91.66%: 100%|██████████| 13/13 [00:00<00:00, 37.95it/s]\n",
            "[5 / 50] Train: Loss = 0.04089, Accuracy = 95.83%: 100%|██████████| 572/572 [00:13<00:00, 43.91it/s]\n",
            "[5 / 50]   Val: Loss = 0.07180, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 39.46it/s]\n",
            "[6 / 50] Train: Loss = 0.03304, Accuracy = 96.59%: 100%|██████████| 572/572 [00:13<00:00, 42.79it/s]\n",
            "[6 / 50]   Val: Loss = 0.07169, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 39.52it/s]\n",
            "[7 / 50] Train: Loss = 0.02713, Accuracy = 97.19%: 100%|██████████| 572/572 [00:13<00:00, 40.92it/s]\n",
            "[7 / 50]   Val: Loss = 0.07090, Accuracy = 92.61%: 100%|██████████| 13/13 [00:00<00:00, 37.14it/s]\n",
            "[8 / 50] Train: Loss = 0.02236, Accuracy = 97.67%: 100%|██████████| 572/572 [00:13<00:00, 40.93it/s]\n",
            "[8 / 50]   Val: Loss = 0.06981, Accuracy = 92.71%: 100%|██████████| 13/13 [00:00<00:00, 37.58it/s]\n",
            "[9 / 50] Train: Loss = 0.01870, Accuracy = 98.06%: 100%|██████████| 572/572 [00:14<00:00, 40.77it/s]\n",
            "[9 / 50]   Val: Loss = 0.07419, Accuracy = 92.76%: 100%|██████████| 13/13 [00:00<00:00, 36.77it/s]\n",
            "[10 / 50] Train: Loss = 0.01538, Accuracy = 98.40%: 100%|██████████| 572/572 [00:13<00:00, 40.92it/s]\n",
            "[10 / 50]   Val: Loss = 0.07942, Accuracy = 92.80%: 100%|██████████| 13/13 [00:00<00:00, 39.31it/s]\n",
            "[11 / 50] Train: Loss = 0.01255, Accuracy = 98.69%: 100%|██████████| 572/572 [00:14<00:00, 40.80it/s]\n",
            "[11 / 50]   Val: Loss = 0.08327, Accuracy = 92.61%: 100%|██████████| 13/13 [00:00<00:00, 38.61it/s]\n",
            "[12 / 50] Train: Loss = 0.01045, Accuracy = 98.93%: 100%|██████████| 572/572 [00:13<00:00, 41.11it/s]\n",
            "[12 / 50]   Val: Loss = 0.08793, Accuracy = 92.69%: 100%|██████████| 13/13 [00:00<00:00, 37.66it/s]\n",
            "[13 / 50] Train: Loss = 0.00857, Accuracy = 99.15%: 100%|██████████| 572/572 [00:13<00:00, 41.13it/s]\n",
            "[13 / 50]   Val: Loss = 0.08628, Accuracy = 92.49%: 100%|██████████| 13/13 [00:00<00:00, 37.17it/s]\n",
            "[14 / 50] Train: Loss = 0.00692, Accuracy = 99.32%: 100%|██████████| 572/572 [00:14<00:00, 40.82it/s]\n",
            "[14 / 50]   Val: Loss = 0.09443, Accuracy = 92.40%: 100%|██████████| 13/13 [00:00<00:00, 37.44it/s]\n",
            "[15 / 50] Train: Loss = 0.00559, Accuracy = 99.49%: 100%|██████████| 572/572 [00:13<00:00, 40.91it/s]\n",
            "[15 / 50]   Val: Loss = 0.09328, Accuracy = 92.41%: 100%|██████████| 13/13 [00:00<00:00, 36.42it/s]\n",
            "[16 / 50] Train: Loss = 0.00454, Accuracy = 99.58%: 100%|██████████| 572/572 [00:13<00:00, 40.99it/s]\n",
            "[16 / 50]   Val: Loss = 0.09776, Accuracy = 92.41%: 100%|██████████| 13/13 [00:00<00:00, 37.72it/s]\n",
            "[17 / 50] Train: Loss = 0.00381, Accuracy = 99.66%: 100%|██████████| 572/572 [00:13<00:00, 41.14it/s]\n",
            "[17 / 50]   Val: Loss = 0.10317, Accuracy = 92.29%: 100%|██████████| 13/13 [00:00<00:00, 39.29it/s]\n",
            "[18 / 50] Train: Loss = 0.00323, Accuracy = 99.70%: 100%|██████████| 572/572 [00:13<00:00, 40.96it/s]\n",
            "[18 / 50]   Val: Loss = 0.10626, Accuracy = 92.40%: 100%|██████████| 13/13 [00:00<00:00, 37.75it/s]\n",
            "[19 / 50] Train: Loss = 0.00262, Accuracy = 99.77%: 100%|██████████| 572/572 [00:13<00:00, 41.18it/s]\n",
            "[19 / 50]   Val: Loss = 0.11481, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 38.27it/s]\n",
            "[20 / 50] Train: Loss = 0.00227, Accuracy = 99.80%: 100%|██████████| 572/572 [00:14<00:00, 40.79it/s]\n",
            "[20 / 50]   Val: Loss = 0.11533, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 39.38it/s]\n",
            "[21 / 50] Train: Loss = 0.00208, Accuracy = 99.81%: 100%|██████████| 572/572 [00:13<00:00, 41.10it/s]\n",
            "[21 / 50]   Val: Loss = 0.12041, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 38.94it/s]\n",
            "[22 / 50] Train: Loss = 0.00223, Accuracy = 99.79%: 100%|██████████| 572/572 [00:13<00:00, 40.95it/s]\n",
            "[22 / 50]   Val: Loss = 0.13044, Accuracy = 92.23%: 100%|██████████| 13/13 [00:00<00:00, 39.38it/s]\n",
            "[23 / 50] Train: Loss = 0.00193, Accuracy = 99.81%: 100%|██████████| 572/572 [00:13<00:00, 41.15it/s]\n",
            "[23 / 50]   Val: Loss = 0.12601, Accuracy = 92.27%: 100%|██████████| 13/13 [00:00<00:00, 38.57it/s]\n",
            "[24 / 50] Train: Loss = 0.00181, Accuracy = 99.82%: 100%|██████████| 572/572 [00:13<00:00, 40.97it/s]\n",
            "[24 / 50]   Val: Loss = 0.12963, Accuracy = 92.39%: 100%|██████████| 13/13 [00:00<00:00, 38.51it/s]\n",
            "[25 / 50] Train: Loss = 0.00179, Accuracy = 99.82%: 100%|██████████| 572/572 [00:13<00:00, 40.93it/s]\n",
            "[25 / 50]   Val: Loss = 0.13196, Accuracy = 92.41%: 100%|██████████| 13/13 [00:00<00:00, 37.99it/s]\n",
            "[26 / 50] Train: Loss = 0.00170, Accuracy = 99.83%: 100%|██████████| 572/572 [00:13<00:00, 41.04it/s]\n",
            "[26 / 50]   Val: Loss = 0.14153, Accuracy = 92.28%: 100%|██████████| 13/13 [00:00<00:00, 39.22it/s]\n",
            "[27 / 50] Train: Loss = 0.00163, Accuracy = 99.83%: 100%|██████████| 572/572 [00:13<00:00, 44.47it/s] \n",
            "[27 / 50]   Val: Loss = 0.13501, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 37.04it/s]\n",
            "[28 / 50] Train: Loss = 0.00176, Accuracy = 99.81%: 100%|██████████| 572/572 [00:13<00:00, 44.40it/s] \n",
            "[28 / 50]   Val: Loss = 0.13369, Accuracy = 92.27%: 100%|██████████| 13/13 [00:00<00:00, 36.62it/s]\n",
            "[29 / 50] Train: Loss = 0.00182, Accuracy = 99.81%: 100%|██████████| 572/572 [00:13<00:00, 40.87it/s]\n",
            "[29 / 50]   Val: Loss = 0.13923, Accuracy = 92.33%: 100%|██████████| 13/13 [00:00<00:00, 38.24it/s]\n",
            "[30 / 50] Train: Loss = 0.00156, Accuracy = 99.83%: 100%|██████████| 572/572 [00:13<00:00, 41.02it/s]\n",
            "[30 / 50]   Val: Loss = 0.14249, Accuracy = 92.36%: 100%|██████████| 13/13 [00:00<00:00, 37.52it/s]\n",
            "[31 / 50] Train: Loss = 0.00146, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 41.03it/s]\n",
            "[31 / 50]   Val: Loss = 0.14047, Accuracy = 92.43%: 100%|██████████| 13/13 [00:00<00:00, 39.15it/s]\n",
            "[32 / 50] Train: Loss = 0.00143, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 40.86it/s]\n",
            "[32 / 50]   Val: Loss = 0.14631, Accuracy = 92.27%: 100%|██████████| 13/13 [00:00<00:00, 36.89it/s]\n",
            "[33 / 50] Train: Loss = 0.00144, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 43.26it/s] \n",
            "[33 / 50]   Val: Loss = 0.14100, Accuracy = 92.31%: 100%|██████████| 13/13 [00:00<00:00, 36.41it/s]\n",
            "[34 / 50] Train: Loss = 0.00162, Accuracy = 99.82%: 100%|██████████| 572/572 [00:14<00:00, 42.20it/s] \n",
            "[34 / 50]   Val: Loss = 0.14828, Accuracy = 92.33%: 100%|██████████| 13/13 [00:00<00:00, 39.14it/s]\n",
            "[35 / 50] Train: Loss = 0.00190, Accuracy = 99.78%: 100%|██████████| 572/572 [00:14<00:00, 42.92it/s] \n",
            "[35 / 50]   Val: Loss = 0.15250, Accuracy = 92.35%: 100%|██████████| 13/13 [00:00<00:00, 38.90it/s]\n",
            "[36 / 50] Train: Loss = 0.00152, Accuracy = 99.83%: 100%|██████████| 572/572 [00:13<00:00, 41.06it/s]\n",
            "[36 / 50]   Val: Loss = 0.14816, Accuracy = 92.32%: 100%|██████████| 13/13 [00:00<00:00, 38.09it/s]\n",
            "[37 / 50] Train: Loss = 0.00136, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 40.93it/s]\n",
            "[37 / 50]   Val: Loss = 0.14943, Accuracy = 92.34%: 100%|██████████| 13/13 [00:00<00:00, 38.68it/s]\n",
            "[38 / 50] Train: Loss = 0.00132, Accuracy = 99.85%: 100%|██████████| 572/572 [00:13<00:00, 41.05it/s]\n",
            "[38 / 50]   Val: Loss = 0.16718, Accuracy = 92.28%: 100%|██████████| 13/13 [00:00<00:00, 39.14it/s]\n",
            "[39 / 50] Train: Loss = 0.00135, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 42.70it/s] \n",
            "[39 / 50]   Val: Loss = 0.16496, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 40.96it/s]\n",
            "[40 / 50] Train: Loss = 0.00145, Accuracy = 99.83%: 100%|██████████| 572/572 [00:13<00:00, 41.15it/s]\n",
            "[40 / 50]   Val: Loss = 0.16787, Accuracy = 92.10%: 100%|██████████| 13/13 [00:00<00:00, 38.83it/s]\n",
            "[41 / 50] Train: Loss = 0.00198, Accuracy = 99.77%: 100%|██████████| 572/572 [00:14<00:00, 40.77it/s]\n",
            "[41 / 50]   Val: Loss = 0.16132, Accuracy = 92.27%: 100%|██████████| 13/13 [00:00<00:00, 37.43it/s]\n",
            "[42 / 50] Train: Loss = 0.00144, Accuracy = 99.83%: 100%|██████████| 572/572 [00:14<00:00, 40.67it/s]\n",
            "[42 / 50]   Val: Loss = 0.16190, Accuracy = 92.37%: 100%|██████████| 13/13 [00:00<00:00, 38.20it/s]\n",
            "[43 / 50] Train: Loss = 0.00127, Accuracy = 99.85%: 100%|██████████| 572/572 [00:13<00:00, 41.08it/s]\n",
            "[43 / 50]   Val: Loss = 0.16781, Accuracy = 92.36%: 100%|██████████| 13/13 [00:00<00:00, 38.21it/s]\n",
            "[44 / 50] Train: Loss = 0.00127, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 41.17it/s]\n",
            "[44 / 50]   Val: Loss = 0.17075, Accuracy = 92.36%: 100%|██████████| 13/13 [00:00<00:00, 38.80it/s]\n",
            "[45 / 50] Train: Loss = 0.00125, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 41.31it/s]\n",
            "[45 / 50]   Val: Loss = 0.15815, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 37.21it/s]\n",
            "[46 / 50] Train: Loss = 0.00126, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 41.06it/s]\n",
            "[46 / 50]   Val: Loss = 0.17209, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 39.37it/s]\n",
            "[47 / 50] Train: Loss = 0.00133, Accuracy = 99.84%: 100%|██████████| 572/572 [00:13<00:00, 40.97it/s]\n",
            "[47 / 50]   Val: Loss = 0.19446, Accuracy = 92.21%: 100%|██████████| 13/13 [00:00<00:00, 39.40it/s]\n",
            "[48 / 50] Train: Loss = 0.00215, Accuracy = 99.75%: 100%|██████████| 572/572 [00:13<00:00, 40.92it/s]\n",
            "[48 / 50]   Val: Loss = 0.17967, Accuracy = 92.18%: 100%|██████████| 13/13 [00:00<00:00, 38.64it/s]\n",
            "[49 / 50] Train: Loss = 0.00143, Accuracy = 99.83%: 100%|██████████| 572/572 [00:13<00:00, 41.02it/s]\n",
            "[49 / 50]   Val: Loss = 0.17778, Accuracy = 92.29%: 100%|██████████| 13/13 [00:00<00:00, 39.41it/s]\n",
            "[50 / 50] Train: Loss = 0.00124, Accuracy = 99.85%: 100%|██████████| 572/572 [00:13<00:00, 41.03it/s]\n",
            "[50 / 50]   Val: Loss = 0.17679, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 37.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3c4f4ea-460a-4b6a-81a5-74cf1ca701dd"
      },
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9251520816334962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGIxc7dDt1A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        out, _ = self._lstm(emb)\n",
        "        return self._out_layer(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyFL_YH342h-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "777db415-944f-4415-8126-4b67e1e63a5e"
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.25903, Accuracy = 76.22%: 100%|██████████| 572/572 [00:16<00:00, 35.85it/s]\n",
            "[1 / 50]   Val: Loss = 0.06658, Accuracy = 89.28%: 100%|██████████| 13/13 [00:00<00:00, 28.25it/s]\n",
            "[2 / 50] Train: Loss = 0.07750, Accuracy = 92.49%: 100%|██████████| 572/572 [00:16<00:00, 34.72it/s]\n",
            "[2 / 50]   Val: Loss = 0.04438, Accuracy = 93.43%: 100%|██████████| 13/13 [00:00<00:00, 28.83it/s]\n",
            "[3 / 50] Train: Loss = 0.04960, Accuracy = 95.33%: 100%|██████████| 572/572 [00:16<00:00, 34.83it/s]\n",
            "[3 / 50]   Val: Loss = 0.03848, Accuracy = 94.86%: 100%|██████████| 13/13 [00:00<00:00, 28.98it/s]\n",
            "[4 / 50] Train: Loss = 0.03505, Accuracy = 96.73%: 100%|██████████| 572/572 [00:16<00:00, 34.85it/s]\n",
            "[4 / 50]   Val: Loss = 0.03377, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 30.08it/s]\n",
            "[5 / 50] Train: Loss = 0.02553, Accuracy = 97.64%: 100%|██████████| 572/572 [00:16<00:00, 34.77it/s]\n",
            "[5 / 50]   Val: Loss = 0.03106, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 29.32it/s]\n",
            "[6 / 50] Train: Loss = 0.01848, Accuracy = 98.31%: 100%|██████████| 572/572 [00:15<00:00, 36.00it/s]\n",
            "[6 / 50]   Val: Loss = 0.03122, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 29.58it/s]\n",
            "[7 / 50] Train: Loss = 0.01336, Accuracy = 98.81%: 100%|██████████| 572/572 [00:15<00:00, 36.60it/s]\n",
            "[7 / 50]   Val: Loss = 0.03231, Accuracy = 96.14%: 100%|██████████| 13/13 [00:00<00:00, 31.16it/s]\n",
            "[8 / 50] Train: Loss = 0.00950, Accuracy = 99.18%: 100%|██████████| 572/572 [00:15<00:00, 36.42it/s]\n",
            "[8 / 50]   Val: Loss = 0.03071, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 30.61it/s]\n",
            "[9 / 50] Train: Loss = 0.00634, Accuracy = 99.48%: 100%|██████████| 572/572 [00:15<00:00, 36.50it/s]\n",
            "[9 / 50]   Val: Loss = 0.03388, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 31.14it/s]\n",
            "[10 / 50] Train: Loss = 0.00415, Accuracy = 99.68%: 100%|██████████| 572/572 [00:15<00:00, 36.23it/s]\n",
            "[10 / 50]   Val: Loss = 0.03536, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 30.17it/s]\n",
            "[11 / 50] Train: Loss = 0.00257, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.16it/s] \n",
            "[11 / 50]   Val: Loss = 0.03747, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 30.39it/s]\n",
            "[12 / 50] Train: Loss = 0.00158, Accuracy = 99.92%: 100%|██████████| 572/572 [00:15<00:00, 36.28it/s] \n",
            "[12 / 50]   Val: Loss = 0.04015, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 31.12it/s]\n",
            "[13 / 50] Train: Loss = 0.00101, Accuracy = 99.95%: 100%|██████████| 572/572 [00:15<00:00, 36.27it/s]\n",
            "[13 / 50]   Val: Loss = 0.04413, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 30.93it/s]\n",
            "[14 / 50] Train: Loss = 0.00061, Accuracy = 99.98%: 100%|██████████| 572/572 [00:16<00:00, 35.65it/s]\n",
            "[14 / 50]   Val: Loss = 0.04735, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 30.38it/s]\n",
            "[15 / 50] Train: Loss = 0.00040, Accuracy = 99.99%: 100%|██████████| 572/572 [00:16<00:00, 40.24it/s] \n",
            "[15 / 50]   Val: Loss = 0.04241, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 30.48it/s]\n",
            "[16 / 50] Train: Loss = 0.00029, Accuracy = 99.99%: 100%|██████████| 572/572 [00:15<00:00, 36.27it/s]\n",
            "[16 / 50]   Val: Loss = 0.04493, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 29.73it/s]\n",
            "[17 / 50] Train: Loss = 0.00116, Accuracy = 99.89%: 100%|██████████| 572/572 [00:15<00:00, 35.93it/s]\n",
            "[17 / 50]   Val: Loss = 0.04709, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 29.91it/s]\n",
            "[18 / 50] Train: Loss = 0.00114, Accuracy = 99.90%: 100%|██████████| 572/572 [00:15<00:00, 36.14it/s]\n",
            "[18 / 50]   Val: Loss = 0.04983, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 30.54it/s]\n",
            "[19 / 50] Train: Loss = 0.00032, Accuracy = 99.99%: 100%|██████████| 572/572 [00:15<00:00, 36.21it/s]\n",
            "[19 / 50]   Val: Loss = 0.04978, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 30.55it/s]\n",
            "[20 / 50] Train: Loss = 0.00014, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.79it/s]\n",
            "[20 / 50]   Val: Loss = 0.04828, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 28.86it/s]\n",
            "[21 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.95it/s]\n",
            "[21 / 50]   Val: Loss = 0.04873, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 29.58it/s]\n",
            "[22 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.85it/s]\n",
            "[22 / 50]   Val: Loss = 0.04930, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 29.37it/s]\n",
            "[23 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.85it/s]\n",
            "[23 / 50]   Val: Loss = 0.05170, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 28.67it/s]\n",
            "[24 / 50] Train: Loss = 0.00122, Accuracy = 99.88%: 100%|██████████| 572/572 [00:15<00:00, 35.82it/s] \n",
            "[24 / 50]   Val: Loss = 0.05232, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 30.56it/s]\n",
            "[25 / 50] Train: Loss = 0.00102, Accuracy = 99.90%: 100%|██████████| 572/572 [00:16<00:00, 35.56it/s]\n",
            "[25 / 50]   Val: Loss = 0.05317, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 28.94it/s]\n",
            "[26 / 50] Train: Loss = 0.00017, Accuracy = 99.99%: 100%|██████████| 572/572 [00:17<00:00, 31.83it/s]\n",
            "[26 / 50]   Val: Loss = 0.05265, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 28.32it/s]\n",
            "[27 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 34.85it/s]\n",
            "[27 / 50]   Val: Loss = 0.05254, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 29.05it/s]\n",
            "[28 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 34.69it/s]\n",
            "[28 / 50]   Val: Loss = 0.05162, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 27.78it/s]\n",
            "[29 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 34.85it/s]\n",
            "[29 / 50]   Val: Loss = 0.05896, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 30.72it/s]\n",
            "[30 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 34.78it/s]\n",
            "[30 / 50]   Val: Loss = 0.05495, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 28.06it/s]\n",
            "[31 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 34.94it/s]\n",
            "[31 / 50]   Val: Loss = 0.05805, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 30.21it/s]\n",
            "[32 / 50] Train: Loss = 0.00152, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 36.33it/s]\n",
            "[32 / 50]   Val: Loss = 0.05557, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 30.21it/s]\n",
            "[33 / 50] Train: Loss = 0.00040, Accuracy = 99.97%: 100%|██████████| 572/572 [00:15<00:00, 36.41it/s]\n",
            "[33 / 50]   Val: Loss = 0.05304, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 29.45it/s]\n",
            "[34 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.94it/s]\n",
            "[34 / 50]   Val: Loss = 0.05452, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 30.50it/s]\n",
            "[35 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.26it/s]\n",
            "[35 / 50]   Val: Loss = 0.05790, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 31.35it/s]\n",
            "[36 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.30it/s]\n",
            "[36 / 50]   Val: Loss = 0.05948, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 31.05it/s]\n",
            "[37 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.27it/s]\n",
            "[37 / 50]   Val: Loss = 0.05773, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 30.50it/s]\n",
            "[38 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.90it/s]\n",
            "[38 / 50]   Val: Loss = 0.05895, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 30.11it/s]\n",
            "[39 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.58it/s]\n",
            "[39 / 50]   Val: Loss = 0.06311, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 31.61it/s]\n",
            "[40 / 50] Train: Loss = 0.00102, Accuracy = 99.90%: 100%|██████████| 572/572 [00:15<00:00, 35.89it/s]\n",
            "[40 / 50]   Val: Loss = 0.06449, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 30.24it/s]\n",
            "[41 / 50] Train: Loss = 0.00067, Accuracy = 99.93%: 100%|██████████| 572/572 [00:15<00:00, 36.22it/s]\n",
            "[41 / 50]   Val: Loss = 0.06758, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 31.10it/s]\n",
            "[42 / 50] Train: Loss = 0.00011, Accuracy = 99.99%: 100%|██████████| 572/572 [00:15<00:00, 36.19it/s]\n",
            "[42 / 50]   Val: Loss = 0.06378, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 29.47it/s]\n",
            "[43 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.40it/s]\n",
            "[43 / 50]   Val: Loss = 0.06562, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 30.69it/s]\n",
            "[44 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.82it/s]\n",
            "[44 / 50]   Val: Loss = 0.06886, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 31.04it/s]\n",
            "[45 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 35.55it/s]\n",
            "[45 / 50]   Val: Loss = 0.06080, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 28.54it/s]\n",
            "[46 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.82it/s]\n",
            "[46 / 50]   Val: Loss = 0.06542, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 29.13it/s]\n",
            "[47 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.10it/s]\n",
            "[47 / 50]   Val: Loss = 0.06743, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 30.58it/s]\n",
            "[48 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 35.84it/s]\n",
            "[48 / 50]   Val: Loss = 0.06772, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 30.97it/s]\n",
            "[49 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:15<00:00, 36.89it/s]\n",
            "[49 / 50]   Val: Loss = 0.07026, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 31.68it/s]\n",
            "[50 / 50] Train: Loss = 0.00040, Accuracy = 99.96%: 100%|██████████| 572/572 [00:15<00:00, 35.88it/s]\n",
            "[50 / 50]   Val: Loss = 0.06958, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 29.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5mutipY8QGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35696dd8-093d-4b1c-c7b5-29a5092f35f8"
      },
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9617721684585963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cc67f4e-d2fe-4c3e-863e-123cc29bf3ee"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0aa98a2c-26a1-4351-c99a-8f8fcb8cf0f1"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding.from_pretrained(embeddings)\n",
        "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        out, _ = self._lstm(emb)\n",
        "        return self._out_layer(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "f642f2f9-d687-440e-8091-f24e8f22b8a1"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.76232, Accuracy = 77.55%: 100%|██████████| 572/572 [00:10<00:00, 54.98it/s]\n",
            "[1 / 50]   Val: Loss = 0.37226, Accuracy = 89.13%: 100%|██████████| 13/13 [00:00<00:00, 36.21it/s]\n",
            "[2 / 50] Train: Loss = 0.28133, Accuracy = 91.61%: 100%|██████████| 572/572 [00:10<00:00, 54.04it/s]\n",
            "[2 / 50]   Val: Loss = 0.25225, Accuracy = 92.29%: 100%|██████████| 13/13 [00:00<00:00, 39.63it/s]\n",
            "[3 / 50] Train: Loss = 0.20456, Accuracy = 93.65%: 100%|██████████| 572/572 [00:11<00:00, 49.12it/s]\n",
            "[3 / 50]   Val: Loss = 0.20851, Accuracy = 93.41%: 100%|██████████| 13/13 [00:00<00:00, 28.22it/s]\n",
            "[4 / 50] Train: Loss = 0.16933, Accuracy = 94.62%: 100%|██████████| 572/572 [00:11<00:00, 54.50it/s]\n",
            "[4 / 50]   Val: Loss = 0.18363, Accuracy = 94.14%: 100%|██████████| 13/13 [00:00<00:00, 38.45it/s]\n",
            "[5 / 50] Train: Loss = 0.14942, Accuracy = 95.15%: 100%|██████████| 572/572 [00:10<00:00, 51.05it/s]\n",
            "[5 / 50]   Val: Loss = 0.16823, Accuracy = 94.46%: 100%|██████████| 13/13 [00:00<00:00, 37.44it/s]\n",
            "[6 / 50] Train: Loss = 0.13636, Accuracy = 95.52%: 100%|██████████| 572/572 [00:10<00:00, 52.71it/s]\n",
            "[6 / 50]   Val: Loss = 0.16156, Accuracy = 94.73%: 100%|██████████| 13/13 [00:00<00:00, 36.59it/s]\n",
            "[7 / 50] Train: Loss = 0.12691, Accuracy = 95.79%: 100%|██████████| 572/572 [00:10<00:00, 52.16it/s]\n",
            "[7 / 50]   Val: Loss = 0.15223, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 34.81it/s]\n",
            "[8 / 50] Train: Loss = 0.12022, Accuracy = 95.95%: 100%|██████████| 572/572 [00:10<00:00, 52.84it/s]\n",
            "[8 / 50]   Val: Loss = 0.14818, Accuracy = 95.05%: 100%|██████████| 13/13 [00:00<00:00, 36.27it/s]\n",
            "[9 / 50] Train: Loss = 0.11465, Accuracy = 96.11%: 100%|██████████| 572/572 [00:10<00:00, 52.61it/s]\n",
            "[9 / 50]   Val: Loss = 0.14408, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 38.32it/s]\n",
            "[10 / 50] Train: Loss = 0.11011, Accuracy = 96.24%: 100%|██████████| 572/572 [00:10<00:00, 52.61it/s]\n",
            "[10 / 50]   Val: Loss = 0.14225, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 37.12it/s]\n",
            "[11 / 50] Train: Loss = 0.10614, Accuracy = 96.35%: 100%|██████████| 572/572 [00:10<00:00, 54.09it/s]\n",
            "[11 / 50]   Val: Loss = 0.13930, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 37.00it/s]\n",
            "[12 / 50] Train: Loss = 0.10308, Accuracy = 96.42%: 100%|██████████| 572/572 [00:10<00:00, 54.24it/s]\n",
            "[12 / 50]   Val: Loss = 0.13849, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 38.16it/s]\n",
            "[13 / 50] Train: Loss = 0.10026, Accuracy = 96.52%: 100%|██████████| 572/572 [00:10<00:00, 55.20it/s]\n",
            "[13 / 50]   Val: Loss = 0.13614, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 39.59it/s]\n",
            "[14 / 50] Train: Loss = 0.09761, Accuracy = 96.58%: 100%|██████████| 572/572 [00:10<00:00, 55.22it/s]\n",
            "[14 / 50]   Val: Loss = 0.13685, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 39.69it/s]\n",
            "[15 / 50] Train: Loss = 0.09549, Accuracy = 96.66%: 100%|██████████| 572/572 [00:10<00:00, 54.77it/s]\n",
            "[15 / 50]   Val: Loss = 0.13454, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 38.78it/s]\n",
            "[16 / 50] Train: Loss = 0.09337, Accuracy = 96.72%: 100%|██████████| 572/572 [00:10<00:00, 54.78it/s]\n",
            "[16 / 50]   Val: Loss = 0.13360, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 36.77it/s]\n",
            "[17 / 50] Train: Loss = 0.09161, Accuracy = 96.78%: 100%|██████████| 572/572 [00:10<00:00, 54.68it/s]\n",
            "[17 / 50]   Val: Loss = 0.13368, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 36.56it/s]\n",
            "[18 / 50] Train: Loss = 0.08965, Accuracy = 96.83%: 100%|██████████| 572/572 [00:10<00:00, 54.74it/s]\n",
            "[18 / 50]   Val: Loss = 0.13458, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 40.85it/s]\n",
            "[19 / 50] Train: Loss = 0.08817, Accuracy = 96.88%: 100%|██████████| 572/572 [00:10<00:00, 55.44it/s]\n",
            "[19 / 50]   Val: Loss = 0.13279, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 37.55it/s]\n",
            "[20 / 50] Train: Loss = 0.08675, Accuracy = 96.93%: 100%|██████████| 572/572 [00:10<00:00, 54.85it/s]\n",
            "[20 / 50]   Val: Loss = 0.13277, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 40.07it/s]\n",
            "[21 / 50] Train: Loss = 0.08543, Accuracy = 96.96%: 100%|██████████| 572/572 [00:10<00:00, 54.14it/s]\n",
            "[21 / 50]   Val: Loss = 0.13352, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 37.48it/s]\n",
            "[22 / 50] Train: Loss = 0.08401, Accuracy = 97.00%: 100%|██████████| 572/572 [00:10<00:00, 55.20it/s]\n",
            "[22 / 50]   Val: Loss = 0.13253, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 38.84it/s]\n",
            "[23 / 50] Train: Loss = 0.08289, Accuracy = 97.06%: 100%|██████████| 572/572 [00:10<00:00, 54.97it/s]\n",
            "[23 / 50]   Val: Loss = 0.13214, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 38.02it/s]\n",
            "[24 / 50] Train: Loss = 0.08169, Accuracy = 97.09%: 100%|██████████| 572/572 [00:10<00:00, 54.74it/s]\n",
            "[24 / 50]   Val: Loss = 0.13195, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 38.63it/s]\n",
            "[25 / 50] Train: Loss = 0.08049, Accuracy = 97.13%: 100%|██████████| 572/572 [00:10<00:00, 55.43it/s]\n",
            "[25 / 50]   Val: Loss = 0.13249, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 39.81it/s]\n",
            "[26 / 50] Train: Loss = 0.07964, Accuracy = 97.16%: 100%|██████████| 572/572 [00:10<00:00, 54.42it/s]\n",
            "[26 / 50]   Val: Loss = 0.13314, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 37.96it/s]\n",
            "[27 / 50] Train: Loss = 0.07853, Accuracy = 97.18%: 100%|██████████| 572/572 [00:10<00:00, 54.97it/s]\n",
            "[27 / 50]   Val: Loss = 0.13293, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 39.64it/s]\n",
            "[28 / 50] Train: Loss = 0.07767, Accuracy = 97.24%: 100%|██████████| 572/572 [00:10<00:00, 54.69it/s]\n",
            "[28 / 50]   Val: Loss = 0.13403, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 39.16it/s]\n",
            "[29 / 50] Train: Loss = 0.07681, Accuracy = 97.26%: 100%|██████████| 572/572 [00:10<00:00, 54.65it/s]\n",
            "[29 / 50]   Val: Loss = 0.13411, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 38.69it/s]\n",
            "[30 / 50] Train: Loss = 0.07582, Accuracy = 97.29%: 100%|██████████| 572/572 [00:10<00:00, 55.38it/s]\n",
            "[30 / 50]   Val: Loss = 0.13351, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 37.27it/s]\n",
            "[31 / 50] Train: Loss = 0.07488, Accuracy = 97.32%: 100%|██████████| 572/572 [00:10<00:00, 54.45it/s]\n",
            "[31 / 50]   Val: Loss = 0.13600, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 36.34it/s]\n",
            "[32 / 50] Train: Loss = 0.07425, Accuracy = 97.33%: 100%|██████████| 572/572 [00:10<00:00, 54.69it/s]\n",
            "[32 / 50]   Val: Loss = 0.13479, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 40.52it/s]\n",
            "[33 / 50] Train: Loss = 0.07368, Accuracy = 97.37%: 100%|██████████| 572/572 [00:10<00:00, 53.36it/s]\n",
            "[33 / 50]   Val: Loss = 0.13543, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 41.12it/s]\n",
            "[34 / 50] Train: Loss = 0.07257, Accuracy = 97.41%: 100%|██████████| 572/572 [00:10<00:00, 54.38it/s]\n",
            "[34 / 50]   Val: Loss = 0.13656, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 39.45it/s]\n",
            "[35 / 50] Train: Loss = 0.07177, Accuracy = 97.44%: 100%|██████████| 572/572 [00:10<00:00, 54.10it/s]\n",
            "[35 / 50]   Val: Loss = 0.13581, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 39.77it/s]\n",
            "[36 / 50] Train: Loss = 0.07124, Accuracy = 97.46%: 100%|██████████| 572/572 [00:10<00:00, 55.18it/s]\n",
            "[36 / 50]   Val: Loss = 0.13692, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 39.44it/s]\n",
            "[37 / 50] Train: Loss = 0.07042, Accuracy = 97.48%: 100%|██████████| 572/572 [00:10<00:00, 56.06it/s]\n",
            "[37 / 50]   Val: Loss = 0.13864, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 38.71it/s]\n",
            "[38 / 50] Train: Loss = 0.06997, Accuracy = 97.50%: 100%|██████████| 572/572 [00:10<00:00, 54.91it/s]\n",
            "[38 / 50]   Val: Loss = 0.13820, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 39.55it/s]\n",
            "[39 / 50] Train: Loss = 0.06906, Accuracy = 97.54%: 100%|██████████| 572/572 [00:10<00:00, 54.90it/s]\n",
            "[39 / 50]   Val: Loss = 0.13747, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 34.91it/s]\n",
            "[40 / 50] Train: Loss = 0.06849, Accuracy = 97.55%: 100%|██████████| 572/572 [00:10<00:00, 54.94it/s]\n",
            "[40 / 50]   Val: Loss = 0.13859, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 38.84it/s]\n",
            "[41 / 50] Train: Loss = 0.06782, Accuracy = 97.57%: 100%|██████████| 572/572 [00:10<00:00, 57.23it/s]\n",
            "[41 / 50]   Val: Loss = 0.13830, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 39.46it/s]\n",
            "[42 / 50] Train: Loss = 0.06722, Accuracy = 97.60%: 100%|██████████| 572/572 [00:10<00:00, 54.23it/s]\n",
            "[42 / 50]   Val: Loss = 0.14014, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 39.01it/s]\n",
            "[43 / 50] Train: Loss = 0.06652, Accuracy = 97.61%: 100%|██████████| 572/572 [00:10<00:00, 51.83it/s]\n",
            "[43 / 50]   Val: Loss = 0.14077, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 41.17it/s]\n",
            "[44 / 50] Train: Loss = 0.06594, Accuracy = 97.65%: 100%|██████████| 572/572 [00:10<00:00, 55.03it/s]\n",
            "[44 / 50]   Val: Loss = 0.14432, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 36.69it/s]\n",
            "[45 / 50] Train: Loss = 0.06558, Accuracy = 97.66%: 100%|██████████| 572/572 [00:10<00:00, 54.26it/s]\n",
            "[45 / 50]   Val: Loss = 0.14356, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 41.09it/s]\n",
            "[46 / 50] Train: Loss = 0.06485, Accuracy = 97.68%: 100%|██████████| 572/572 [00:10<00:00, 54.71it/s]\n",
            "[46 / 50]   Val: Loss = 0.14206, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 36.65it/s]\n",
            "[47 / 50] Train: Loss = 0.06423, Accuracy = 97.70%: 100%|██████████| 572/572 [00:10<00:00, 55.31it/s]\n",
            "[47 / 50]   Val: Loss = 0.14315, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 39.68it/s]\n",
            "[48 / 50] Train: Loss = 0.06404, Accuracy = 97.71%: 100%|██████████| 572/572 [00:10<00:00, 54.70it/s]\n",
            "[48 / 50]   Val: Loss = 0.14390, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 39.92it/s]\n",
            "[49 / 50] Train: Loss = 0.06325, Accuracy = 97.76%: 100%|██████████| 572/572 [00:10<00:00, 55.78it/s]\n",
            "[49 / 50]   Val: Loss = 0.14690, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 39.07it/s]\n",
            "[50 / 50] Train: Loss = 0.06267, Accuracy = 97.76%: 100%|██████████| 572/572 [00:10<00:00, 55.27it/s]\n",
            "[50 / 50]   Val: Loss = 0.14793, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 40.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "188f0555-e149-4dc5-a44c-ebe3fc2158ae"
      },
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9532549084411762"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoxlK4Zf8Ybq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}